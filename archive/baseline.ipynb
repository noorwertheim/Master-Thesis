{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from CNN_classifier_model import CNNClassifier, train_model\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path\n",
    "from preprocessing_modules import create_time_windows_with_labels\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "wandb.login(key=\"5f15eb7efc1e0e939ccc83345338a0b8c24e2fbc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"tcn-time-series-classification\", config={\n",
    "    \"model\": \"TCN\",\n",
    "    \"input_length\": 12000,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"kernel_size\": 7,\n",
    "    \"channels\": [32, 64],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(univariate_data_path, \"target_univariate_train.npy\")\n",
    "train_data = np.load(train_dir, allow_pickle=True)\n",
    "# train_data = pd.DataFrame(train_data)\n",
    "test_dir = os.path.join(univariate_data_path, \"target_univariate_test.npy\")\n",
    "test_data = np.load(test_dir, allow_pickle=True)\n",
    "# test_data = pd.DataFrame(test_data)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train_data[0])\n",
    "\n",
    "train_windows = create_time_windows_with_labels(train_data)\n",
    "test_windows = create_time_windows_with_labels(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Example dataset with windows and labels\n",
    "train_windows, train_labels = create_time_windows_with_labels(train_data)\n",
    "test_windows, test_labels = create_time_windows_with_labels(test_data)\n",
    "print(train_windows.shape, train_labels.shape)\n",
    "# Count label distribution\n",
    "train_label_counts = Counter(train_labels)\n",
    "test_label_counts = Counter(test_labels)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Train label distribution:\")\n",
    "print(f\"  Term (0): {train_label_counts[0]}\")\n",
    "print(f\"  Preterm (1): {train_label_counts[1]}\")\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(f\"  Term (0): {test_label_counts[0]}\")\n",
    "print(f\"  Preterm (1): {test_label_counts[1]}\")\n",
    "\n",
    "# Convert to tensors\n",
    "train_windows_tensor = torch.tensor(train_windows, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_windows_tensor = torch.tensor(test_windows, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_windows_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_windows_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plottin example training instance'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select an index to inspect\n",
    "idx = 20 \n",
    "\n",
    "window, label = train_dataset[idx]\n",
    "window_np = window.numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(window_np, linewidth=0.5)\n",
    "plt.title(f\"Train Sample {idx} - Label: {'Preterm (1)' if label == 1 else 'Term (0)'}\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Signal Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label for sample {idx}: {label.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Remove the extra padding on the right (to keep causality).\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size]\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCNClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, num_channels=[32, 64], kernel_size=7, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i - 1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                                     dilation=dilation_size, padding=(kernel_size - 1) * dilation_size,\n",
    "                                     dropout=dropout)]\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], 1)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (B, 1, 12000) for 1D conv\n",
    "        y = self.tcn(x)     # (B, C, L)\n",
    "        y = y.mean(dim=2)   # Global average pooling over time\n",
    "        out = self.classifier(y)\n",
    "        return out.squeeze(1)  # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TCNClassifier().to(device)\n",
    "wandb.watch(model, log='all', log_freq=10)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()  # outputs logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    # Train\n",
    "    train_loss = train(model, train_loader, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item() * batch_x.size(0)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": avg_test_loss,\n",
    "        \"test_accuracy\": acc,\n",
    "        \"test_f1\": f1,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "    # Plot loss curves and log the figure to wandb\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(1, epoch + 2), test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    wandb.log({\"Loss Curve\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Test Loss={avg_test_loss:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
