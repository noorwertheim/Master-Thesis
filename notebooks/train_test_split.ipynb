{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'record_name': 'ice001_l_1of1-chan0', 'signal': array([[-1.02738069],\n",
      "       [ 2.41713519],\n",
      "       [ 1.36444037],\n",
      "       ...,\n",
      "       [-4.87554464],\n",
      "       [-4.33903399],\n",
      "       [-5.24322553]]), 'metadata': {'fs': 20, 'sig_len': 100000, 'n_sig': 16, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'comments': ['Info:', 'ID:ice001', 'Record type:labour', 'Record number:1/1', 'Age(years):31', 'BMI before pregnancy:23.3', 'BMI at recording:27.6', 'Gravidity:3', 'Parity:2', 'Previous caesarean:No', 'Placental position:Fundus', 'Gestational age at recording(w/d):39/3', 'Gestational age at delivery:39/3', 'Mode of delivery:Vaginal', 'Synthetic oxytocin use in labour:No', 'Epidural during labour:No', 'Comments for recording:', 'Electrodes placed 5-10 mins prior to beginning of recording.', 'Baby born 20 minutes after the end of the recording.']}}\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'merged_univariate_no_PCA'\n",
    "save_dir = os.path.join(univariate_data_path, dataset_name + \".npy\")\n",
    "dataset = np.load(save_dir, allow_pickle=True)\n",
    "\n",
    "if dataset_name == 'target_univariate_no_PCA': \n",
    "    print('Number of instances in target data: ', len(dataset))\n",
    "    dataset = [item for item in dataset if item['preterm'] is not None]\n",
    "    print('Number of labeled instances in target data: ', len(dataset))\n",
    "\n",
    "print(dataset[0])\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seed = 42\n",
    "# random.seed(random_seed)\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# random.shuffle(dataset)\n",
    "\n",
    "# # Split 80-20\n",
    "# split_idx = int(0.8 * len(dataset))\n",
    "# train_data = dataset[:split_idx]\n",
    "# test_data = dataset[split_idx:]\n",
    "\n",
    "# # Save to .npy files\n",
    "# train_dir = os.path.join(univariate_data_path, dataset_name + \"_train.npy\")\n",
    "# test_dir = os.path.join(univariate_data_path, dataset_name + \"_test.npy\")\n",
    "\n",
    "# np.save(train_dir, train_data)\n",
    "# np.save(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 4003 samples\n",
      "Test set: 1171 samples\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose `data` is your list of dicts\n",
    "# Step 1: Group by base record ID\n",
    "grouped = defaultdict(list)\n",
    "for entry in dataset:\n",
    "    base_id = \"-\".join(entry['record_name'].split(\"-\")[:-1])  # remove '-chanX'\n",
    "    grouped[base_id].append(entry)\n",
    "\n",
    "# Step 2: Get all unique base IDs\n",
    "all_ids = list(grouped.keys())\n",
    "\n",
    "# Step 3: Split base IDs into train and test (80/20)\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Reconstruct datasets\n",
    "train_data = [entry for id_ in train_ids for entry in grouped[id_]]\n",
    "test_data  = [entry for id_ in test_ids for entry in grouped[id_]]\n",
    "\n",
    "print(f\"Train set: {len(train_data)} samples\")\n",
    "print(f\"Test set: {len(test_data)} samples\")\n",
    "\n",
    "# print(train_set[0])\n",
    "train_dir = os.path.join(univariate_data_path, dataset_name + \"_train.npy\")\n",
    "test_dir = os.path.join(univariate_data_path, dataset_name + \"_test.npy\")\n",
    "\n",
    "np.save(train_dir, train_data)\n",
    "np.save(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
