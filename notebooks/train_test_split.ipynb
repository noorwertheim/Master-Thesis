{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in target data:  1008\n",
      "Number of labeled instances in target data:  936\n",
      "{'record_name': 'Hopper-2023_05_25_12_03_38-0000010090-0003-chan0', 'signal': array([[-1.70320952],\n",
      "       [-3.41875192],\n",
      "       [-2.48399423],\n",
      "       ...,\n",
      "       [ 0.73116872],\n",
      "       [ 0.4095313 ],\n",
      "       [ 0.95039407]]), 'fs': 20, 'preterm': 0}\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'target_univariate_no_PCA'\n",
    "save_dir = os.path.join(univariate_data_path, dataset_name + \".npy\")\n",
    "dataset = np.load(save_dir, allow_pickle=True)\n",
    "\n",
    "if dataset_name == 'target_univariate_no_PCA': \n",
    "    print('Number of instances in target data: ', len(dataset))\n",
    "    dataset = [item for item in dataset if item['preterm'] is not None]\n",
    "    print('Number of labeled instances in target data: ', len(dataset))\n",
    "\n",
    "print(dataset[0])\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seed = 42\n",
    "# random.seed(random_seed)\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# random.shuffle(dataset)\n",
    "\n",
    "# # Split 80-20\n",
    "# split_idx = int(0.8 * len(dataset))\n",
    "# train_data = dataset[:split_idx]\n",
    "# test_data = dataset[split_idx:]\n",
    "\n",
    "# # Save to .npy files\n",
    "# train_dir = os.path.join(univariate_data_path, dataset_name + \"_train.npy\")\n",
    "# test_dir = os.path.join(univariate_data_path, dataset_name + \"_test.npy\")\n",
    "\n",
    "# np.save(train_dir, train_data)\n",
    "# np.save(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 744 samples\n",
      "Test set: 192 samples\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose `data` is your list of dicts\n",
    "# Step 1: Group by base record ID\n",
    "grouped = defaultdict(list)\n",
    "for entry in dataset:\n",
    "    base_id = \"-\".join(entry['record_name'].split(\"-\")[:-1])  # remove '-chanX'\n",
    "    grouped[base_id].append(entry)\n",
    "\n",
    "# Step 2: Get all unique base IDs\n",
    "all_ids = list(grouped.keys())\n",
    "\n",
    "# Step 3: Split base IDs into train and test (80/20)\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.2, random_state=43)\n",
    "\n",
    "# Step 4: Reconstruct datasets\n",
    "train_data = [entry for id_ in train_ids for entry in grouped[id_]]\n",
    "test_data  = [entry for id_ in test_ids for entry in grouped[id_]]\n",
    "\n",
    "print(f\"Train set: {len(train_data)} samples\")\n",
    "print(f\"Test set: {len(test_data)} samples\")\n",
    "\n",
    "# print(train_set[0])\n",
    "train_dir = os.path.join(univariate_data_path, dataset_name + \"_train_2.npy\")\n",
    "test_dir = os.path.join(univariate_data_path, dataset_name + \"_test_2.npy\")\n",
    "\n",
    "np.save(train_dir, train_data)\n",
    "np.save(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "Saved 20% subset to ../data/univariate/target_univariate_no_PCA_train_2_20.npy\n",
      "297\n",
      "Saved 40% subset to ../data/univariate/target_univariate_no_PCA_train_2_40.npy\n",
      "446\n",
      "Saved 60% subset to ../data/univariate/target_univariate_no_PCA_train_2_60.npy\n",
      "595\n",
      "Saved 80% subset to ../data/univariate/target_univariate_no_PCA_train_2_80.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the original dataset\n",
    "dataset_name = 'target_univariate_no_PCA_train_2'\n",
    "save_dir = os.path.join(univariate_data_path, dataset_name + \".npy\")\n",
    "dataset = np.load(save_dir, allow_pickle=True)\n",
    "\n",
    "# Set the split percentages\n",
    "percentages = [20, 40, 60, 80]\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = np.random.permutation(len(dataset))\n",
    "\n",
    "# Generate and save the subsets\n",
    "for pct in percentages:\n",
    "    subset_size = int(len(dataset) * pct / 100)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    subset = dataset[subset_indices]\n",
    "    print(len(subset))\n",
    "\n",
    "    subset_name = f\"{dataset_name}_{pct}\"\n",
    "    subset_save_path = os.path.join(univariate_data_path, subset_name + \".npy\")\n",
    "    np.save(subset_save_path, subset)\n",
    "    print(f\"Saved {pct}% subset to {subset_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
