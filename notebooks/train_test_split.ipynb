{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in target data:  1002\n",
      "Number of labeled instances in target data:  858\n",
      "{'record_name': 'Hopper-2023_05_25_12_03_38-0000010090-0003-chan0', 'signal': array([[-1.70320951],\n",
      "       [-3.4187519 ],\n",
      "       [-2.48399421],\n",
      "       ...,\n",
      "       [ 0.73116871],\n",
      "       [ 0.4095313 ],\n",
      "       [ 0.95039407]]), 'fs': 20, 'preterm': 0}\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'target_univariate_no_PCA'\n",
    "save_dir = os.path.join(univariate_data_path, dataset_name + \".npy\")\n",
    "dataset = np.load(save_dir, allow_pickle=True)\n",
    "\n",
    "if dataset_name == 'target_univariate_no_PCA': \n",
    "    print('Number of instances in target data: ', len(dataset))\n",
    "    dataset = [item for item in dataset if item['preterm'] is not None]\n",
    "    print('Number of labeled instances in target data: ', len(dataset))\n",
    "\n",
    "print(dataset[0])\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seed = 42\n",
    "# random.seed(random_seed)\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# random.shuffle(dataset)\n",
    "\n",
    "# # Split 80-20\n",
    "# split_idx = int(0.8 * len(dataset))\n",
    "# train_data = dataset[:split_idx]\n",
    "# test_data = dataset[split_idx:]\n",
    "\n",
    "# # Save to .npy files\n",
    "# train_dir = os.path.join(univariate_data_path, dataset_name + \"_train.npy\")\n",
    "# test_dir = os.path.join(univariate_data_path, dataset_name + \"_test.npy\")\n",
    "\n",
    "# np.save(train_dir, train_data)\n",
    "# np.save(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 684 samples\n",
      "Test set: 174 samples\n",
      "{'record_name': 'Hopper-2023_03_06_19_35_18-0000010181-0000-chan0', 'signal': array([[30.26377405],\n",
      "       [51.72742573],\n",
      "       [38.71189734],\n",
      "       ...,\n",
      "       [-1.23572809],\n",
      "       [ 2.92797998],\n",
      "       [-4.08031036]]), 'fs': 20, 'preterm': 0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose `data` is your list of dicts\n",
    "# Step 1: Group by base record ID\n",
    "grouped = defaultdict(list)\n",
    "for entry in dataset:\n",
    "    base_id = \"-\".join(entry['record_name'].split(\"-\")[:-1])  # remove '-chanX'\n",
    "    grouped[base_id].append(entry)\n",
    "\n",
    "# Step 2: Get all unique base IDs\n",
    "all_ids = list(grouped.keys())\n",
    "\n",
    "# Step 3: Split base IDs into train and test (80/20)\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Reconstruct datasets\n",
    "train_data = [entry for id_ in train_ids for entry in grouped[id_]]\n",
    "test_data  = [entry for id_ in test_ids for entry in grouped[id_]]\n",
    "\n",
    "print(f\"Train set: {len(train_set)} samples\")\n",
    "print(f\"Test set: {len(test_set)} samples\")\n",
    "\n",
    "print(train_set[0])\n",
    "train_dir = os.path.join(univariate_data_path, dataset_name + \"_train.npy\")\n",
    "test_dir = os.path.join(univariate_data_path, dataset_name + \"_test.npy\")\n",
    "\n",
    "np.save(train_dir, train_data)\n",
    "np.save(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
