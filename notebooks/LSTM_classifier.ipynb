{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size=500, step=250):\n",
    "    all_windows, labels, record_names = [], [], []\n",
    "    \n",
    "    for sample in data:\n",
    "        signal, label, record_name = sample[\"signal\"], sample[\"preterm\"], sample[\"record_name\"]\n",
    "        \n",
    "        # Ensure the label is a valid integer\n",
    "        if label is None:  # Skip instances with None labels\n",
    "            continue\n",
    "        \n",
    "        for start in range(0, len(signal) - window_size + 1, step):\n",
    "            window = signal[start : start + window_size]\n",
    "            all_windows.append(window)\n",
    "            labels.append(label)\n",
    "            record_names.append(record_name)\n",
    "    \n",
    "    return np.array(all_windows, dtype=np.float32), np.array(labels, dtype=np.int32), record_names\n",
    "\n",
    "# Example usage:\n",
    "x_train, y_train, record_names = create_windows(target_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class LSTMClassifier:\n",
    "    def __init__(self, encoder, input_shape, num_classes, lstm_units=64):\n",
    "        self.encoder = encoder\n",
    "        self.model = self._build_classifier(input_shape, num_classes, lstm_units)\n",
    "    \n",
    "    def _build_classifier(self, input_shape, num_classes, lstm_units):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "        # Freeze encoder weights\n",
    "        self.encoder.trainable = False\n",
    "\n",
    "        # Pass each time step through the encoder separately\n",
    "        encoded_sequence = layers.TimeDistributed(self.encoder)(inputs)\n",
    "\n",
    "        # LSTM layers for sequential modeling\n",
    "        x = layers.LSTM(lstm_units, return_sequences=True)(encoded_sequence)\n",
    "        x = layers.LSTM(lstm_units, return_sequences=False)(x)  # Get final output\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        x = layers.Dense(16, activation=\"relu\")(x)\n",
    "        outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        model = models.Model(inputs, outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=64, epochs=50, validation_split=0.1):\n",
    "        history = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group windows by record name\n",
    "grouped_windows = defaultdict(list)\n",
    "for i, record_name in enumerate(record_names):\n",
    "    grouped_windows[record_name].append(x_train[i])\n",
    "\n",
    "# Now you can concatenate the windows for each record\n",
    "concatenated_windows = []\n",
    "for record_name, windows in grouped_windows.items():\n",
    "    # Concatenate windows of the same record\n",
    "    concatenated_windows.append(np.concatenate(windows, axis=0))  # Shape: (num_windows * window_size, )\n",
    "\n",
    "# Convert to numpy array for use in the model\n",
    "concatenated_windows = np.array(concatenated_windows)\n",
    "# Reshape data for LSTM (add feature dimension if needed)\n",
    "concatenated_windows = concatenated_windows.reshape(concatenated_windows.shape[0], -1, 1)  # (batch_size, sequence_length, features)\n",
    "\n",
    "# Now use the LSTM classifier to train on these windows\n",
    "classifier = Classifier(encoder, input_shape=(concatenated_windows.shape[1], 1), num_classes=num_classes)\n",
    "classifier.train(concatenated_windows, y_train)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
