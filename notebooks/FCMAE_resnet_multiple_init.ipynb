{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nwertheim/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "from preprocessing_modules import create_time_windows_with_labels, create_time_windows_with_metadata\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import matplotlib.pyplot as plt\n",
    "from FCMAE_model import FCMAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FCMAE Hyperparameters:\n",
    "batch_size = 16\n",
    "num_blocks = 5\n",
    "kernel_size = 7\n",
    "base_dim = 32\n",
    "learning_rate_FCMAE = 0.00016938000495408888\n",
    "input_dimension = 512\n",
    "\n",
    "# ResNet classification head hyperparameters (SGD)\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n",
      "192\n",
      "{'record_name': 'Hopper-2024_11_03_14_13_16-0000010179-0002-chan0', 'signal': array([[ 0.70569938],\n",
      "       [ 1.58579907],\n",
      "       [ 1.22047813],\n",
      "       ...,\n",
      "       [-0.30476434],\n",
      "       [-0.16021862],\n",
      "       [-0.40381152]]), 'fs': 20, 'preterm': 1}\n",
      "torch.Size([9900])\n",
      "batch size 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''MY DATA + class weights'''\n",
    "train_dir = os.path.join(univariate_data_path, \"target_univariate_no_PCA_train_2.npy\")\n",
    "train_data = np.load(train_dir, allow_pickle=True)\n",
    "# train_data = pd.DataFrame(train_data)\n",
    "test_dir = os.path.join(univariate_data_path, \"target_univariate_no_PCA_test_2.npy\")\n",
    "test_data = np.load(test_dir, allow_pickle=True)\n",
    "# test_data = pd.DataFrame(test_data)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train_data[0])\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import SGD\n",
    "from torch.optim import AdamW\n",
    "from functools import partial\n",
    "from fastai.optimizer import OptimWrapper\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_windows, train_labels, train_meta = create_time_windows_with_metadata(train_data)\n",
    "test_windows, test_labels, test_meta = create_time_windows_with_metadata(test_data)\n",
    "\n",
    "# Convert to tensors\n",
    "train_windows_tensor = torch.tensor(train_windows, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_windows_tensor = torch.tensor(test_windows, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "train_windows_tensor = train_windows_tensor.unsqueeze(1)  # (1071, 1, 12000)\n",
    "test_windows_tensor = test_windows_tensor.unsqueeze(1)    # (899, 1, 12000)\n",
    "\n",
    "train_labels_tensor = train_labels_tensor.long()\n",
    "test_labels_tensor = test_labels_tensor.long()\n",
    "\n",
    "\n",
    "print(train_labels_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(train_windows_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_windows_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Test metadata for reference\n",
    "test_meta = test_meta\n",
    "print('batch size', batch_size)\n",
    "dls = DataLoaders(train_loader, test_loader)\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/nwertheim.11777034/ipykernel_2461142/2144079635.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m fcmae = FCMAE(in_channels=\u001b[32m1\u001b[39m, base_dim=base_dim, num_blocks=num_blocks, kernel_size=kernel_size)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load the saved weights into the full model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m state_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m fcmae.load_state_dict(state_dict)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Extract only the encoder from the FCMAE model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1360\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1358\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1359\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1368\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1848\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   1847\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1851\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1812\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1784\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   1779\u001b[39m         storage.byteswap(dtype)\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   1783\u001b[39m typed_storage = torch.storage.TypedStorage(\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     wrap_storage=\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1785\u001b[39m     dtype=dtype,\n\u001b[32m   1786\u001b[39m     _internal=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1787\u001b[39m )\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typed_storage._data_ptr() != \u001b[32m0\u001b[39m:\n\u001b[32m   1790\u001b[39m     loaded_storages[key] = typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:601\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    583\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:539\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    537\u001b[39m     backend_name = torch._C._get_privateuse1_backend_name()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     device = \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:508\u001b[39m, in \u001b[36m_validate_device\u001b[39m\u001b[34m(location, backend_name)\u001b[39m\n\u001b[32m    506\u001b[39m     device_index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mis_available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.is_available() is False. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mdevice_count\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    516\u001b[39m     device_count = device_module.device_count()\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained encoder\n",
    "path = os.path.join(models_path, 'FCMAE_encoder_no_PCA_gpu_normalized_correct.pth')\n",
    "\n",
    "# Load the full FCMAE model\n",
    "fcmae = FCMAE(in_channels=1, base_dim=base_dim, num_blocks=num_blocks, kernel_size=kernel_size)\n",
    "\n",
    "\n",
    "# Load the saved weights into the full model\n",
    "state_dict = torch.load(path)\n",
    "fcmae.load_state_dict(state_dict)\n",
    "\n",
    "# Extract only the encoder from the FCMAE model\n",
    "encoder = fcmae.encoder\n",
    "# Set the encoder to evaluation mode and freeze the encoder parameters\n",
    "encoder.eval()\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False  # Freeze the encoder\n",
    "\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, average_precision_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import SGD\n",
    "from fastai.metrics import accuracy\n",
    "from tsai.data.core import DataLoaders\n",
    "\n",
    "class FCMAEClassifier(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = ResNet(2048, 2)  # 2048 channels from encoder, 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # freeze encoder\n",
    "            x = self.encoder(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ----------------------------- Utility Functions -----------------------------\n",
    "def seed_everything(seed):\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ----------------------------- Experiment Setup -----------------------------\n",
    "num_epochs = 20\n",
    "num_runs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_train_losses = []\n",
    "all_valid_losses = []\n",
    "all_metrics = []\n",
    "\n",
    "best_ap = -1\n",
    "best_preds = None\n",
    "best_targs = None\n",
    "\n",
    "all_pred_labels = []\n",
    "all_pred_probs = []\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=class_weights_tensor) \n",
    "\n",
    "\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\nðŸŒ€ Run {run+1}/{num_runs}\")\n",
    "    seed_everything(run)\n",
    "\n",
    "    model = FCMAEClassifier(encoder).to(device)\n",
    "\n",
    "    learn = Learner(dls, model, loss_func=loss_func, metrics=accuracy, opt_func=SGD)\n",
    "    learn.model.to(device)\n",
    "\n",
    "    learn.fit_one_cycle(num_epochs, learning_rate)\n",
    "\n",
    "    # Record losses\n",
    "    losses = np.array(learn.recorder.losses)\n",
    "    iters_per_epoch = len(dls.train)\n",
    "    train_loss = losses[::2][:num_epochs]\n",
    "    valid_loss = losses[1::2][:num_epochs]\n",
    "    all_train_losses.append(train_loss)\n",
    "    all_valid_losses.append(valid_loss)\n",
    "\n",
    "    # Predictions\n",
    "    preds, targs = learn.get_preds(dl=dls.valid)\n",
    "    pred_labels = preds.argmax(dim=1)\n",
    "    pred_probs = preds[:, 1].cpu().numpy()\n",
    "    true_labels = targs.cpu().numpy()\n",
    "\n",
    "    all_pred_labels.append(pred_labels.cpu().numpy())\n",
    "    all_pred_probs.append(pred_probs)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    prec = precision_score(true_labels, pred_labels, zero_division=0)\n",
    "    rec = recall_score(true_labels, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, zero_division=0)\n",
    "    ap = average_precision_score(true_labels, pred_probs)\n",
    "    auc = roc_auc_score(true_labels, pred_probs)\n",
    "    all_metrics.append([acc, prec, rec, f1, ap, auc])\n",
    "\n",
    "    if ap > best_ap:\n",
    "        best_ap = ap\n",
    "        best_preds = preds\n",
    "        best_targs = true_labels\n",
    "\n",
    "# ----------------------------- Post-Processing -----------------------------\n",
    "all_train_losses = np.stack(all_train_losses)\n",
    "all_valid_losses = np.stack(all_valid_losses)\n",
    "all_metrics = np.stack(all_metrics)\n",
    "\n",
    "# Average + std metrics\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AP', 'AUC']\n",
    "mean_metrics = all_metrics.mean(axis=0)\n",
    "std_metrics = all_metrics.std(axis=0)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Evaluation:\")\n",
    "for name, mean, std in zip(metric_names, mean_metrics, std_metrics):\n",
    "    print(f\"{name}: {mean:.4f} Â± {std:.4f}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_labels = best_preds.argmax(dim=1).cpu().numpy()\n",
    "pred_probs = torch.softmax(best_preds, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "# Assuming `test_meta` has the record_name and window_index\n",
    "window_ids = [entry['record_name'] + '-' + str(entry['window_index']) for entry in test_meta]\n",
    "final_df = pd.DataFrame({\n",
    "    'window_id': window_ids,\n",
    "    'best_pred_label': pred_labels,\n",
    "    'best_pred_prob': pred_probs,\n",
    "    'target': best_targs\n",
    "})\n",
    "\n",
    "for run_idx, (labels, probs) in enumerate(zip(all_pred_labels, all_pred_probs), start=1):\n",
    "    final_df[f'pred_label{run_idx}'] = labels\n",
    "    final_df[f'pred_prob{run_idx}'] = probs\n",
    "\n",
    "final_df.to_csv(\"results_FCMAE_resnet_SGD.csv\", index=False)\n",
    "\n",
    "# ----------------------------- Plot Loss Curves -----------------------------\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "mean_train = all_train_losses.mean(axis=0)\n",
    "std_train = all_train_losses.std(axis=0)\n",
    "plt.plot(epochs, mean_train, label='Train Loss')\n",
    "plt.fill_between(epochs, mean_train - std_train, mean_train + std_train, alpha=0.3)\n",
    "\n",
    "mean_valid = all_valid_losses.mean(axis=0)\n",
    "std_valid = all_valid_losses.std(axis=0)\n",
    "plt.plot(epochs, mean_valid, label='Valid Loss')\n",
    "plt.fill_between(epochs, mean_valid - std_valid, mean_valid + std_valid, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Average Loss Curves Over Five Runs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------- Plot Metrics -----------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(metric_names, mean_metrics, yerr=std_metrics, capsize=5)\n",
    "plt.title('Average Performance Over Five Runs')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------- Confusion Matrix -----------------------------\n",
    "cm = confusion_matrix(best_targs, pred_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.title('Best Performing Model Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
