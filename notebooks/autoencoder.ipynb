{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nwertheim/Master-Thesis/envnoor/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666,)\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(univariate_data_path, 'merged_univariate.npy')\n",
    "data = np.load(data_file, allow_pickle=True)\n",
    "print(data.shape)\n",
    "#test\n",
    "\n",
    "\n",
    "# Check for NaN or Inf values in signals\n",
    "for sample in data:\n",
    "    if np.isnan(sample['signal']).any() or np.isinf(sample['signal']).any():\n",
    "        print(f\"NaN or Inf detected in {sample['record_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(sequence, window_size, step_size):\n",
    "    windows = [sequence[i:i+window_size] for i in range(0, len(sequence) - window_size + 1, step_size)]\n",
    "    return np.array(windows)\n",
    "\n",
    "# Set windowing parameters\n",
    "window_size = 500  \n",
    "step_size = 250    \n",
    "\n",
    "# Process each record and apply windowing\n",
    "all_windows = [create_windows(record['signal'], window_size, step_size) for record in data]\n",
    "all_windows = np.concatenate(all_windows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mask a percentage of data in each window\n",
    "def mask_data(windows, mask_percentage=0.2):\n",
    "    masked_windows = windows.copy()\n",
    "    num_masked = int(mask_percentage * windows.shape[1])\n",
    "    for i in range(windows.shape[0]):\n",
    "        mask_indices = np.random.choice(windows.shape[1], num_masked, replace=False)\n",
    "        masked_windows[i, mask_indices] = np.nan  \n",
    "    return np.nan_to_num(masked_windows, nan=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for final autoencoder\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class MaskedAutoencoder(tf.keras.Model):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(MaskedAutoencoder, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder = models.Sequential([\n",
    "#             layers.Dense(128),\n",
    "#             layers.ReLU(),\n",
    "#             layers.Dense(64),\n",
    "#             layers.ReLU(),\n",
    "#         ])\n",
    "\n",
    "#         # Decoder\n",
    "#         self.decoder = models.Sequential([\n",
    "#             layers.Dense(128),\n",
    "#             layers.ReLU(),\n",
    "#             layers.Dense(input_dim, activation='linear')  # Reconstruct original input\n",
    "#         ])\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         encoded = self.encoder(inputs)\n",
    "#         decoded = self.decoder(encoded)\n",
    "#         return decoded\n",
    "\n",
    "# # Set input shape\n",
    "# input_dim = 500  # Each window has 500 elements\n",
    "# autoencoder = MaskedAutoencoder(input_dim)\n",
    "\n",
    "# # Use Adam optimizer with lower learning rate and gradient clipping\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
    "\n",
    "# autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# # Train the model and store history\n",
    "# history = autoencoder.fit(masked_windows, all_windows, epochs=20, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# # Plot loss per epoch\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='dashed')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Autoencoder Training Loss per Epoch')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "# # Save the encoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# import matplotlib.pyplot as plt\n",
    "# from skopt import gp_minimize\n",
    "# from skopt.space import Real, Integer, Categorical\n",
    "# from skopt.utils import use_named_args\n",
    "\n",
    "# # Hyperparameter search space\n",
    "# search_space = [\n",
    "#     Real(0.1, 0.75, name='masking_ratio'),  # Masking ratio (10% - 75%)\n",
    "#     Real(1e-5, 1e-2, \"log-uniform\", name='learning_rate'),  # Learning rate\n",
    "#     Categorical([16, 32, 64], name='batch_size'),  # Only powers of 2\n",
    "#     Integer(1, 5, name='num_layers'),  # Number of hidden layers (1 to 5)\n",
    "#     Integer(32, 256, name='num_nodes')  # Number of nodes in each layer (32 to 256)\n",
    "# ]\n",
    "\n",
    "# class MaskedAutoencoder(tf.keras.Model):\n",
    "#     def __init__(self, input_dim, num_layers, num_nodes):\n",
    "#         super(MaskedAutoencoder, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder = models.Sequential()\n",
    "#         for _ in range(num_layers):\n",
    "#             self.encoder.add(layers.Dense(num_nodes, activation='relu'))\n",
    "\n",
    "#         # Decoder\n",
    "#         self.decoder = models.Sequential()\n",
    "#         for _ in range(num_layers):\n",
    "#             self.decoder.add(layers.Dense(num_nodes, activation='relu'))\n",
    "#         self.decoder.add(layers.Dense(input_dim, activation='linear'))  # Reconstruct original input\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         encoded = self.encoder(inputs)\n",
    "#         decoded = self.decoder(encoded)\n",
    "#         return decoded\n",
    "\n",
    "# # File to save results\n",
    "# results_file = \"bayesian_optimization_results.txt\"\n",
    "# plot_file = \"bayesian_optimization_plot.png\"\n",
    "\n",
    "# def save_results(text):\n",
    "#     with open(results_file, \"a\") as f:\n",
    "#         f.write(text + \"\\n\")\n",
    "\n",
    "# # Optimization function\n",
    "# @use_named_args(search_space)\n",
    "# def objective(masking_ratio, learning_rate, batch_size, num_layers, num_nodes):\n",
    "#     result_text = f\"\\nTesting Hyperparameter Combination:\\n\"\n",
    "#     result_text += f\" - Masking Ratio: {masking_ratio:.2f}\\n\"\n",
    "#     result_text += f\" - Learning Rate: {learning_rate:.5f}\\n\"\n",
    "#     result_text += f\" - Batch Size: {batch_size}\\n\"\n",
    "#     result_text += f\" - Number of Layers: {num_layers}\\n\"\n",
    "#     result_text += f\" - Number of Nodes: {num_nodes}\\n\"\n",
    "#     print(result_text)\n",
    "#     save_results(result_text)\n",
    "\n",
    "#     # Apply masking\n",
    "#     masked_windows = mask_data(all_windows, mask_percentage=masking_ratio)\n",
    "\n",
    "#     # Build and compile model\n",
    "#     autoencoder = MaskedAutoencoder(input_dim=window_size, num_layers=num_layers, num_nodes=num_nodes)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "#     autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "#     # Train the model\n",
    "#     history = autoencoder.fit(masked_windows, all_windows, epochs=10, batch_size=batch_size, validation_split=0.1, verbose=0)\n",
    "\n",
    "#     final_loss = history.history['val_loss'][-1]\n",
    "#     result_text = f\" - Validation Loss: {final_loss:.5f}\\n\"\n",
    "#     print(result_text)\n",
    "#     save_results(result_text)\n",
    "\n",
    "#     return final_loss\n",
    "\n",
    "# # Run Bayesian Optimization\n",
    "# result = gp_minimize(objective, search_space, n_calls=20, random_state=42)\n",
    "\n",
    "# # Save and print best hyperparameters\n",
    "# best_params_text = \"\\nBest Hyperparameters:\\n\"\n",
    "# best_params_text += f\"Masking Ratio: {result.x[0]:.2f}\\n\"\n",
    "# best_params_text += f\"Learning Rate: {result.x[1]:.5f}\\n\"\n",
    "# best_params_text += f\"Batch Size: {result.x[2]}\\n\"\n",
    "# best_params_text += f\"Number of Layers: {result.x[3]}\\n\"\n",
    "# best_params_text += f\"Number of Nodes: {result.x[4]}\\n\"\n",
    "# print(best_params_text)\n",
    "# save_results(best_params_text)\n",
    "\n",
    "# # Plot optimization results\n",
    "# plt.plot(result.func_vals)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Validation Loss (MSE)\")\n",
    "# plt.title(\"Bayesian Optimization Progress\")\n",
    "# plt.savefig(plot_file)  # Save the plot as an image\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.62\n",
      " - Learning Rate: 0.00004\n",
      " - Batch Size: 64\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:35:53.772792: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Validation Loss: 1.60723\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.39\n",
      " - Learning Rate: 0.00002\n",
      " - Batch Size: 32\n",
      " - Number of Layers: 3\n",
      " - Number of Nodes Per Layer: [128, 64, 32]\n",
      "\n",
      " - Validation Loss: 1.04106\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.19\n",
      " - Learning Rate: 0.00090\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.44836\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.71\n",
      " - Learning Rate: 0.00001\n",
      " - Batch Size: 64\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 1.82547\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.50\n",
      " - Learning Rate: 0.00001\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 1.50447\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.36\n",
      " - Learning Rate: 0.00001\n",
      " - Batch Size: 64\n",
      " - Number of Layers: 3\n",
      " - Number of Nodes Per Layer: [128, 64, 32]\n",
      "\n",
      " - Validation Loss: 1.70943\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.16\n",
      " - Learning Rate: 0.00072\n",
      " - Batch Size: 32\n",
      " - Number of Layers: 5\n",
      " - Number of Nodes Per Layer: [512, 256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.45194\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.40\n",
      " - Learning Rate: 0.00380\n",
      " - Batch Size: 64\n",
      " - Number of Layers: 3\n",
      " - Number of Nodes Per Layer: [128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.82243\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.11\n",
      " - Learning Rate: 0.00671\n",
      " - Batch Size: 32\n",
      " - Number of Layers: 3\n",
      " - Number of Nodes Per Layer: [128, 64, 32]\n",
      "\n",
      " - Validation Loss: 1.62866\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.11\n",
      " - Learning Rate: 0.00005\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.10570\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.10\n",
      " - Learning Rate: 0.00005\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.08468\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.26\n",
      " - Learning Rate: 0.00018\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 2\n",
      " - Number of Nodes Per Layer: [64, 32]\n",
      "\n",
      " - Validation Loss: 0.11832\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.10\n",
      " - Learning Rate: 0.00007\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 4\n",
      " - Number of Nodes Per Layer: [256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.06881\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.45\n",
      " - Learning Rate: 0.00063\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 2\n",
      " - Number of Nodes Per Layer: [64, 32]\n",
      "\n",
      " - Validation Loss: 0.17173\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.75\n",
      " - Learning Rate: 0.00744\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 2\n",
      " - Number of Nodes Per Layer: [64, 32]\n",
      "\n",
      " - Validation Loss: 1.59434\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.74\n",
      " - Learning Rate: 0.00037\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 2\n",
      " - Number of Nodes Per Layer: [64, 32]\n",
      "\n",
      " - Validation Loss: 0.34902\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.10\n",
      " - Learning Rate: 0.00011\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 2\n",
      " - Number of Nodes Per Layer: [64, 32]\n",
      "\n",
      " - Validation Loss: 0.09309\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.10\n",
      " - Learning Rate: 0.00046\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 5\n",
      " - Number of Nodes Per Layer: [512, 256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.32151\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.41\n",
      " - Learning Rate: 0.00032\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 5\n",
      " - Number of Nodes Per Layer: [512, 256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.30389\n",
      "\n",
      "\n",
      "Testing Hyperparameter Combination:\n",
      " - Masking Ratio: 0.10\n",
      " - Learning Rate: 0.00007\n",
      " - Batch Size: 16\n",
      " - Number of Layers: 5\n",
      " - Number of Nodes Per Layer: [512, 256, 128, 64, 32]\n",
      "\n",
      " - Validation Loss: 0.11706\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m best_params_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m best_params_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 96\u001b[0m best_params_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Nodes Per Layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m-\u001b[39mresult\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m3\u001b[39m]:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_params_text)\n\u001b[1;32m     98\u001b[0m save_results(best_params_text)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = [\n",
    "    Real(0.1, 0.75, name='masking_ratio'),\n",
    "    Real(1e-5, 1e-2, \"log-uniform\", name='learning_rate'),\n",
    "    Categorical([16, 32, 64], name='batch_size'),\n",
    "    Integer(2, 5, name='num_layers'),  # Changed from 1-5 to 2-5\n",
    "]\n",
    "\n",
    "class MaskedAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, input_dim, num_layers, num_nodes_per_layer):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "\n",
    "        num_nodes_per_layer = list(num_nodes_per_layer)[-num_layers:]  # Ensure correct selection\n",
    "\n",
    "        # Encoder (decreasing nodes per layer)\n",
    "        self.encoder = models.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.encoder.add(layers.Dense(num_nodes_per_layer[i], activation='relu'))\n",
    "\n",
    "        # Decoder (increasing nodes per layer, mirroring encoder)\n",
    "        self.decoder = models.Sequential()\n",
    "        for i in range(num_layers - 1, -1, -1):\n",
    "            self.decoder.add(layers.Dense(num_nodes_per_layer[i], activation='relu'))\n",
    "        self.decoder.add(layers.Dense(input_dim, activation='linear'))  # Reconstruct original input\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# File to save results\n",
    "results_file = \"bayesian_optimization_results.txt\"\n",
    "plot_file = \"bayesian_optimization_plot.png\"\n",
    "\n",
    "def save_results(text):\n",
    "    with open(results_file, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Optimization function\n",
    "@use_named_args(search_space)\n",
    "def objective(masking_ratio, learning_rate, batch_size, num_layers):\n",
    "    num_nodes_per_layer = []\n",
    "    num_nodes = [32, 64, 128, 256, 512]\n",
    "    for i in range(num_layers):\n",
    "        num_nodes_per_layer.append(num_nodes[i]) \n",
    "    num_nodes_per_layer.reverse()\n",
    "\n",
    "    # num_nodes_per_layer = list(num_nodes_per_layer)\n",
    "    # if len(num_nodes_per_layer) < num_layers:\n",
    "    #     num_nodes_per_layer = num_nodes_per_layer + [num_nodes_per_layer[-1]] * (num_layers - len(num_nodes_per_layer))\n",
    "\n",
    "\n",
    "    result_text = f\"\\nTesting Hyperparameter Combination:\\n\"\n",
    "    result_text += f\" - Masking Ratio: {masking_ratio:.2f}\\n\"\n",
    "    result_text += f\" - Learning Rate: {learning_rate:.5f}\\n\"\n",
    "    result_text += f\" - Batch Size: {batch_size}\\n\"\n",
    "    result_text += f\" - Number of Layers: {num_layers}\\n\"\n",
    "    result_text += f\" - Number of Nodes Per Layer: {num_nodes_per_layer}\\n\"\n",
    "    print(result_text)\n",
    "    save_results(result_text)\n",
    "\n",
    "    # Apply masking\n",
    "    masked_windows = mask_data(all_windows, mask_percentage=masking_ratio)\n",
    "\n",
    "    # Build and compile model\n",
    "    autoencoder = MaskedAutoencoder(input_dim=window_size, num_layers=num_layers, num_nodes_per_layer=num_nodes_per_layer)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = autoencoder.fit(masked_windows, all_windows, epochs=10, batch_size=batch_size, validation_split=0.1, verbose=0)\n",
    "\n",
    "    final_loss = history.history['val_loss'][-1]\n",
    "    result_text = f\" - Validation Loss: {final_loss:.5f}\\n\"\n",
    "    print(result_text)\n",
    "    save_results(result_text)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "result = gp_minimize(objective, search_space, n_calls=20, random_state=42)\n",
    "\n",
    "# Save and print best hyperparameters\n",
    "best_params_text = \"\\nBest Hyperparameters:\\n\"\n",
    "best_params_text += f\"Masking Ratio: {result.x[0]:.2f}\\n\"\n",
    "best_params_text += f\"Learning Rate: {result.x[1]:.5f}\\n\"\n",
    "best_params_text += f\"Batch Size: {result.x[2]}\\n\"\n",
    "best_params_text += f\"Number of Layers: {result.x[3]}\\n\"\n",
    "best_params_text += f\"Number of Nodes Per Layer: {list(result.x[4])[-result.x[3]:]}\\n\"\n",
    "print(best_params_text)\n",
    "save_results(best_params_text)\n",
    "\n",
    "# Plot optimization results\n",
    "plt.plot(result.func_vals)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.title(\"Bayesian Optimization Progress\")\n",
    "plt.savefig(plot_file)  # Save the plot as an image\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39msave(path)  \u001b[38;5;66;03m# Saves in Keras format\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved encoder!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "# path = os.path.join(models_path, 'encoder_model.keras')\n",
    "# autoencoder.encoder.save(path)  # Saves in Keras format\n",
    "# print('Saved encoder!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
