{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges all source data into one .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ehgdb1, shape: (976,)\n",
      "Loaded ehgdb2, shape: (992,)\n",
      "Loaded icehgds, shape: (378,)\n",
      "Loaded nifeadb, shape: (155,)\n",
      "Loaded ninfea, shape: (1680,)\n",
      "Loaded tpehgdb, shape: (900,)\n",
      "Loaded tpehgt, shape: (93,)\n",
      "Saved merged dataset to ../data/univariate/merged_univariate_no_PCA.npy, shape: (5174,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = ['ehgdb1', 'ehgdb2', 'icehgds', 'nifeadb', 'ninfea', 'tpehgdb', 'tpehgt']\n",
    "data_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    file_path = os.path.join(univariate_data_path, dataset + \"_univariate_no_PCA.npy\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_list.append(data)\n",
    "        print(f\"Loaded {dataset}, shape: {data.shape}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Merge all datasets into one large NumPy array\n",
    "if data_list:\n",
    "    merged_data = np.concatenate(data_list, axis=0)  # Assuming they have the same structure\n",
    "    output_file = os.path.join(univariate_data_path, \"merged_univariate_no_PCA.npy\")\n",
    "    \n",
    "    np.save(output_file, merged_data)\n",
    "    print(f\"Saved merged dataset to {output_file}, shape: {merged_data.shape}\")\n",
    "else:\n",
    "    print(\"No data loaded. Check file paths.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: ../data/univariate/merged_univariate_no_PCA.npy\n",
      "Total instances: 5174\n",
      "\n",
      "First 3 entries:\n",
      "\n",
      "Entry 1:\n",
      "  Record Name: ice001_l_1of1-chan0\n",
      "  Signal Shape: (7600, 1)\n",
      "{'record_name': 'ice001_l_1of1-chan0', 'signal': array([[-1.02738069],\n",
      "       [ 2.41713519],\n",
      "       [ 1.36444037],\n",
      "       ...,\n",
      "       [-4.87554464],\n",
      "       [-4.33903399],\n",
      "       [-5.24322553]]), 'metadata': {'fs': 20, 'sig_len': 100000, 'n_sig': 16, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'comments': ['Info:', 'ID:ice001', 'Record type:labour', 'Record number:1/1', 'Age(years):31', 'BMI before pregnancy:23.3', 'BMI at recording:27.6', 'Gravidity:3', 'Parity:2', 'Previous caesarean:No', 'Placental position:Fundus', 'Gestational age at recording(w/d):39/3', 'Gestational age at delivery:39/3', 'Mode of delivery:Vaginal', 'Synthetic oxytocin use in labour:No', 'Epidural during labour:No', 'Comments for recording:', 'Electrodes placed 5-10 mins prior to beginning of recording.', 'Baby born 20 minutes after the end of the recording.']}}\n",
      "\n",
      "Entry 2:\n",
      "  Record Name: ice001_l_1of1-chan1\n",
      "  Signal Shape: (7600, 1)\n",
      "{'record_name': 'ice001_l_1of1-chan1', 'signal': array([[-2.09750419],\n",
      "       [-2.86004513],\n",
      "       [-2.29754119],\n",
      "       ...,\n",
      "       [-1.01601841],\n",
      "       [-1.16870479],\n",
      "       [-0.91234058]]), 'metadata': {'fs': 20, 'sig_len': 100000, 'n_sig': 16, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'comments': ['Info:', 'ID:ice001', 'Record type:labour', 'Record number:1/1', 'Age(years):31', 'BMI before pregnancy:23.3', 'BMI at recording:27.6', 'Gravidity:3', 'Parity:2', 'Previous caesarean:No', 'Placental position:Fundus', 'Gestational age at recording(w/d):39/3', 'Gestational age at delivery:39/3', 'Mode of delivery:Vaginal', 'Synthetic oxytocin use in labour:No', 'Epidural during labour:No', 'Comments for recording:', 'Electrodes placed 5-10 mins prior to beginning of recording.', 'Baby born 20 minutes after the end of the recording.']}}\n",
      "\n",
      "Entry 3:\n",
      "  Record Name: ice001_l_1of1-chan2\n",
      "  Signal Shape: (7600, 1)\n",
      "{'record_name': 'ice001_l_1of1-chan2', 'signal': array([[ 1.322215  ],\n",
      "       [ 4.5072422 ],\n",
      "       [ 3.20528774],\n",
      "       ...,\n",
      "       [-2.47450259],\n",
      "       [-1.94343306],\n",
      "       [-2.83798553]]), 'metadata': {'fs': 20, 'sig_len': 100000, 'n_sig': 16, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'comments': ['Info:', 'ID:ice001', 'Record type:labour', 'Record number:1/1', 'Age(years):31', 'BMI before pregnancy:23.3', 'BMI at recording:27.6', 'Gravidity:3', 'Parity:2', 'Previous caesarean:No', 'Placental position:Fundus', 'Gestational age at recording(w/d):39/3', 'Gestational age at delivery:39/3', 'Mode of delivery:Vaginal', 'Synthetic oxytocin use in labour:No', 'Epidural during labour:No', 'Comments for recording:', 'Electrodes placed 5-10 mins prior to beginning of recording.', 'Baby born 20 minutes after the end of the recording.']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the merged dataset\n",
    "merged_file = os.path.join(univariate_data_path, \"merged_univariate_no_PCA.npy\")\n",
    "\n",
    "if os.path.exists(merged_file):\n",
    "    merged_data = np.load(merged_file, allow_pickle=True)\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(f\"Dataset loaded successfully: {merged_file}\")\n",
    "    print(f\"Total instances: {len(merged_data)}\")\n",
    "    \n",
    "    # Print first few entries as a \"header\"\n",
    "    print(\"\\nFirst 3 entries:\")\n",
    "    for i, entry in enumerate(merged_data[:3]):\n",
    "        print(f\"\\nEntry {i+1}:\")\n",
    "        print(f\"  Record Name: {entry['record_name']}\")\n",
    "        print(f\"  Signal Shape: {entry['signal'].shape}\")\n",
    "        print(entry)\n",
    "        # print(f\"  Metadata: {entry['metadata']}\")\n",
    "else:\n",
    "    print(f\"File not found: {merged_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
