{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nwertheim/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import matplotlib.pyplot as plt\n",
    "from FCMAE_model import FCMAE, ConvNeXtBlock1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_epochs = 5\n",
    "num_blocks = 5\n",
    "kernel_size = 9\n",
    "base_dim = 128\n",
    "learning_rate = 3.57844559759971e-05\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4003,)\n",
      "(1171,)\n",
      "(33060, 1)\n",
      "{'record_name': 'icehg956-chan0', 'signal': array([[ 3.15229875],\n",
      "       [ 2.9328331 ],\n",
      "       [ 2.72123653],\n",
      "       ...,\n",
      "       [-0.50077395],\n",
      "       [-0.52593061],\n",
      "       [-0.54714237]]), 'metadata': {'fs': 20, 'sig_len': 35460, 'n_sig': 6, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'sig_name': ['S1', 'S1_DOCFILT-4-0.08-5', 'S2', 'S2_DOCFILT-4-0.08-5', 'S3', 'S3_DOCFILT-4-0.08-5'], 'comments': ['Comments:', 'RecID 956', 'RecType Induced', 'Gestation 40', 'Rectime 30.3', 'Age 28', 'Weight 82', 'Placental_position end', 'Height 173', 'Newborn_weight 3350', 'Pair_RecID 958']}}\n"
     ]
    }
   ],
   "source": [
    "train_file = os.path.join(univariate_data_path, 'merged_univariate_no_PCA_train.npy')\n",
    "train_data = np.load(train_file, allow_pickle=True)\n",
    "test_file = os.path.join(univariate_data_path, 'merged_univariate_no_PCA_test.npy')\n",
    "test_data = np.load(test_file, allow_pickle=True)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print(train_data[0]['signal'].shape)\n",
    "print(train_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9669, 12000)\n",
      "(3179, 12000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create time windows from the signal data\n",
    "def create_time_windows(data, window_length=12000, step_size=12000):\n",
    "    \"\"\"\n",
    "    Create sliding windows of a specified length and step size from a list of signals.\n",
    "\n",
    "    Args:\n",
    "        data: list of dicts, each containing a 'signal' array of shape (seq_len, 1)\n",
    "        window_length: number of time steps in each window\n",
    "        step_size: number of time steps to move between windows (for overlap)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: shape (num_windows, window_length), univariate windows\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "\n",
    "    for entry in data:\n",
    "        signal = entry['signal'].flatten()  # shape: (seq_len,)\n",
    "        signal_length = len(signal)\n",
    "\n",
    "        if signal_length < window_length:\n",
    "            continue\n",
    "\n",
    "        for start_idx in range(0, signal_length - window_length + 1, step_size):\n",
    "            end_idx = start_idx + window_length\n",
    "            window = signal[start_idx:end_idx]\n",
    "            windows.append(window)\n",
    "\n",
    "    return np.array(windows)  # shape: (num_windows, window_length)\n",
    "\n",
    "\n",
    "X_train = create_time_windows(train_data)\n",
    "X_test = create_time_windows(test_data)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Training set shape: (9669, 12000)\n",
      "Masked Test set shape: (3179, 12000)\n"
     ]
    }
   ],
   "source": [
    "def mask_data(x, mask_ratio=0.75, patch_size=8):\n",
    "    \"\"\"\n",
    "    Apply patch-based masking to a batch of univariate time series.\n",
    "\n",
    "    Args:\n",
    "        x: np.ndarray of shape (num_windows, window_length)\n",
    "        mask_ratio: float, fraction of patches to mask\n",
    "        patch_size: int, number of time steps in each patch\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: masked version of x with same shape\n",
    "    \"\"\"\n",
    "    x_masked = np.copy(x)\n",
    "    num_windows, window_length = x.shape\n",
    "    num_patches = window_length // patch_size\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        mask = np.random.rand(num_patches) < mask_ratio\n",
    "        for j in range(num_patches):\n",
    "            if mask[j]:\n",
    "                start = j * patch_size\n",
    "                end = (j + 1) * patch_size\n",
    "                x_masked[i, start:end] = 0  # or np.nan if you prefer\n",
    "\n",
    "    return x_masked\n",
    "\n",
    "# Apply masking to train, validation, and test sets\n",
    "mask_ratio = 0.75  # Adjust this to your desired masking ratio\n",
    "masked_X_train = mask_data(X_train, mask_ratio)\n",
    "masked_X_test = mask_data(X_test, mask_ratio)\n",
    "\n",
    "# Check the shape of the masked datasets\n",
    "print(f\"Masked Training set shape: {masked_X_train.shape}\")\n",
    "print(f\"Masked Test set shape: {masked_X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define batch size\n",
    "batch_size = batch_size\n",
    "\n",
    "# Convert to tensors (still on CPU at this point)\n",
    "masked_X_train_tensor = torch.tensor(masked_X_train, dtype=torch.float32).unsqueeze(1)\n",
    "unmasked_X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "masked_X_test_tensor = torch.tensor(masked_X_test, dtype=torch.float32).unsqueeze(1)\n",
    "unmasked_X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets with both masked and unmasked tensors\n",
    "train_dataset = TensorDataset(masked_X_train_tensor, unmasked_X_train_tensor)\n",
    "test_dataset = TensorDataset(masked_X_test_tensor, unmasked_X_test_tensor)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/5 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_repeats):\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_repeats\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     train_losses, test_losses, model = \u001b[43mtrain_and_evaluate_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     all_train_losses.append(train_losses)\n\u001b[32m     76\u001b[39m     all_test_losses.append(test_losses)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mtrain_and_evaluate_once\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m     loss.backward()\n\u001b[32m     56\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m avg_train_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     61\u001b[39m train_losses.append(avg_train_loss)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seeds for more reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "num_repeats = 5  # How many times to repeat training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_and_evaluate_once():\n",
    "    model = FCMAE(in_channels=1, base_dim=base_dim, num_blocks=num_blocks, kernel_size=kernel_size)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- EVALUATE ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for masked_input, original_input in test_loader:\n",
    "                masked_input = masked_input.to(device)\n",
    "                original_input = original_input.to(device)\n",
    "\n",
    "                reconstructed = model(masked_input)\n",
    "                loss = criterion(reconstructed, original_input)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "            test_losses.append(avg_test_loss)\n",
    "\n",
    "        # --- TRAIN ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for masked_input, original_input in train_loader:\n",
    "            masked_input = masked_input.to(device)\n",
    "            original_input = original_input.to(device)\n",
    "\n",
    "            reconstructed = model(masked_input)\n",
    "            loss = criterion(reconstructed, original_input)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "    return train_losses, test_losses, model\n",
    "\n",
    "\n",
    "# Store all runs\n",
    "all_train_losses = []\n",
    "all_test_losses = []\n",
    "final_test_losses = []\n",
    "models = []\n",
    "\n",
    "for run in range(num_repeats):\n",
    "    print(f\"\\n=== Run {run+1}/{num_repeats} ===\")\n",
    "    train_losses, test_losses, model = train_and_evaluate_once()\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_test_losses.append(test_losses)\n",
    "    final_test_losses.append(test_losses[-1])\n",
    "    models.append(model)\n",
    "\n",
    "# Convert to numpy arrays for easier computation\n",
    "all_train_losses = np.array(all_train_losses)\n",
    "all_test_losses = np.array(all_test_losses)\n",
    "final_test_losses = np.array(final_test_losses)\n",
    "\n",
    "# Compute mean and std\n",
    "mean_train_losses = np.mean(all_train_losses, axis=0)\n",
    "std_train_losses = np.std(all_train_losses, axis=0)\n",
    "mean_test_losses = np.mean(all_test_losses, axis=0)\n",
    "std_test_losses = np.std(all_test_losses, axis=0)\n",
    "\n",
    "# --- Plotting ---\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mean_train_losses, label=\"Mean Train Loss\", color=\"blue\", marker='o')\n",
    "plt.fill_between(epochs, \n",
    "                 mean_train_losses - std_train_losses, \n",
    "                 mean_train_losses + std_train_losses, \n",
    "                 color=\"blue\", alpha=0.2)\n",
    "\n",
    "plt.plot(epochs, mean_test_losses, label=\"Mean Test Loss\", color=\"red\", marker='x')\n",
    "plt.fill_between(epochs, \n",
    "                 mean_test_losses - std_test_losses, \n",
    "                 mean_test_losses + std_test_losses, \n",
    "                 color=\"red\", alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Train and Test Loss over {num_repeats} Runs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print summary ---\n",
    "print(\"\\nFinal Test Loss per run:\")\n",
    "for i, loss in enumerate(final_test_losses):\n",
    "    print(f\"Run {i+1}: Final Test Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Final Test Loss: {np.mean(final_test_losses):.4f}\")\n",
    "print(f\"Std of Final Test Loss: {np.std(final_test_losses):.4f}\")\n",
    "\n",
    "# --- Saving the best model ---\n",
    "best_run_idx = np.argmin(final_test_losses)  # Run with lowest final test loss\n",
    "best_model = models[best_run_idx]\n",
    "\n",
    "# Save only the encoder part\n",
    "encoder_path = os.path.join(models_path, 'FCMAE_encoder_no_PCA_gpu_normalized.pth')\n",
    "torch.save(best_model.state_dict(), encoder_path)\n",
    "\n",
    "print(f\"\\nBest model was from Run {best_run_idx+1} with Final Test Loss = {final_test_losses[best_run_idx]:.4f}\")\n",
    "print(f\"Model encoder saved to {encoder_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
