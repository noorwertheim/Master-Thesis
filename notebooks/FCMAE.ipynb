{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nwertheim/miniconda3/bin/python\n",
      "/home/nwertheim/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import matplotlib.pyplot as plt\n",
    "from FCMAE_model import FCMAE, ConvNeXtBlock1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "num_blocks = 5\n",
    "kernel_size = 9\n",
    "base_dim = 128\n",
    "learning_rate = 5.493606514151052e-05\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532,)\n",
      "(134,)\n",
      "(72200, 1)\n",
      "{'record_name': 'ice002_p_2of3', 'signal': array([[-12.15116958],\n",
      "       [-24.48972151],\n",
      "       [-18.22349939],\n",
      "       ...,\n",
      "       [  3.40956282],\n",
      "       [  1.22642183],\n",
      "       [  4.90008321]]), 'metadata': {'fs': 20, 'sig_len': 746000, 'n_sig': 16, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'comments': ['Info:', 'ID:ice002', 'Record type:pregnancy', 'Record number:2/3', 'Age(years):38', 'BMI before pregnancy:20.7', 'BMI at recording:25.9', 'Gravidity:4', 'Parity:1', 'Previous caesarean:No', 'Placental position:Posterior', 'Gestational age at recording(w/d):39/1', 'Gestational age at delivery:40/4', 'Mode of delivery:Vaginal', 'Synthetic oxytocin use in labour:No', 'Epidural during labour:No', 'Comments for recording:', 'Reference on right hip - apparent reverse ECG.', 'Comments for delivery:']}}\n"
     ]
    }
   ],
   "source": [
    "train_file = os.path.join(univariate_data_path, 'merged_univariate_train.npy')\n",
    "train_data = np.load(train_file, allow_pickle=True)\n",
    "test_file = os.path.join(univariate_data_path, 'merged_univariate_test.npy')\n",
    "test_data = np.load(test_file, allow_pickle=True)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print(train_data[0]['signal'].shape)\n",
    "print(train_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1230, 12000)\n",
      "(320, 12000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create time windows from the signal data\n",
    "def create_time_windows(data, window_length=12000, step_size=12000):\n",
    "    \"\"\"\n",
    "    Create sliding windows of a specified length and step size from a list of signals.\n",
    "\n",
    "    Args:\n",
    "        data: list of dicts, each containing a 'signal' array of shape (seq_len, 1)\n",
    "        window_length: number of time steps in each window\n",
    "        step_size: number of time steps to move between windows (for overlap)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: shape (num_windows, window_length), univariate windows\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "\n",
    "    for entry in data:\n",
    "        signal = entry['signal'].flatten()  # shape: (seq_len,)\n",
    "        signal_length = len(signal)\n",
    "\n",
    "        if signal_length < window_length:\n",
    "            continue\n",
    "\n",
    "        for start_idx in range(0, signal_length - window_length + 1, step_size):\n",
    "            end_idx = start_idx + window_length\n",
    "            window = signal[start_idx:end_idx]\n",
    "            windows.append(window)\n",
    "\n",
    "    return np.array(windows)  # shape: (num_windows, window_length)\n",
    "\n",
    "\n",
    "X_train = create_time_windows(train_data)\n",
    "X_test = create_time_windows(test_data)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def split_data(windows, test_size=0.1, val_size=0.1, random_seed=42):\n",
    "#     \"\"\"\n",
    "#     Splits the dataset into training, validation, and test sets with specified proportions.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - windows (numpy.ndarray): The dataset to split, shaped as (num_samples, window_size).\n",
    "#     - test_size (float): Proportion of data to be used for the test set.\n",
    "#     - val_size (float): Proportion of data to be used for the validation set.\n",
    "#     - random_seed (int): Seed for reproducibility.\n",
    "\n",
    "#     Returns:\n",
    "#     - X_train, X_val, X_test: The splits of the dataset.\n",
    "#     \"\"\"\n",
    "#     # First, split into train and temp (test + validation)\n",
    "#     X_train, X_temp = train_test_split(windows, test_size=(test_size + val_size), random_state=random_seed)\n",
    "    \n",
    "#     # Then, split the temp into validation and test\n",
    "#     val_size_adjusted = val_size / (val_size + test_size)  # Adjust to split remaining temp\n",
    "#     X_val, X_test = train_test_split(X_temp, test_size=val_size_adjusted, random_state=random_seed)\n",
    "    \n",
    "#     return X_train, X_val, X_test\n",
    "\n",
    "# # Example usage with your windows data\n",
    "# X_train, X_val, X_test = split_data(windows, test_size=0.1, val_size=0.1)\n",
    "\n",
    "# # Check the shapes of the splits\n",
    "# print(f\"Training set shape: {X_train.shape}\")\n",
    "# print(f\"Validation set shape: {X_val.shape}\")\n",
    "# print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Training set shape: (1230, 12000)\n",
      "Masked Test set shape: (320, 12000)\n"
     ]
    }
   ],
   "source": [
    "def mask_data(x, mask_ratio=0.5, patch_size=8):\n",
    "    \"\"\"\n",
    "    Apply patch-based masking to a batch of univariate time series.\n",
    "\n",
    "    Args:\n",
    "        x: np.ndarray of shape (num_windows, window_length)\n",
    "        mask_ratio: float, fraction of patches to mask\n",
    "        patch_size: int, number of time steps in each patch\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: masked version of x with same shape\n",
    "    \"\"\"\n",
    "    x_masked = np.copy(x)\n",
    "    num_windows, window_length = x.shape\n",
    "    num_patches = window_length // patch_size\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        mask = np.random.rand(num_patches) < mask_ratio\n",
    "        for j in range(num_patches):\n",
    "            if mask[j]:\n",
    "                start = j * patch_size\n",
    "                end = (j + 1) * patch_size\n",
    "                x_masked[i, start:end] = 0  # or np.nan if you prefer\n",
    "\n",
    "    return x_masked\n",
    "\n",
    "# Apply masking to train, validation, and test sets\n",
    "mask_ratio = 0.75  # Adjust this to your desired masking ratio\n",
    "masked_X_train = mask_data(X_train, mask_ratio)\n",
    "masked_X_test = mask_data(X_test, mask_ratio)\n",
    "\n",
    "# Check the shape of the masked datasets\n",
    "print(f\"Masked Training set shape: {masked_X_train.shape}\")\n",
    "print(f\"Masked Test set shape: {masked_X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_masked_examples(original, masked, num_examples=5):\n",
    "#     plt.figure(figsize=(15, num_examples * 2.5))\n",
    "    \n",
    "#     for i in range(num_examples):\n",
    "#         # Original signal\n",
    "#         plt.subplot(num_examples, 2, 2*i + 1)\n",
    "#         plt.plot(original[i], color='blue')\n",
    "#         plt.title(f\"Original Signal {i+1}\")\n",
    "#         plt.xlabel(\"Time step\")\n",
    "#         plt.ylabel(\"Amplitude\")\n",
    "\n",
    "#         # Masked signal\n",
    "#         plt.subplot(num_examples, 2, 2*i + 2)\n",
    "#         plt.plot(masked[i], color='orange')\n",
    "#         plt.title(f\"Masked Signal {i+1}\")\n",
    "#         plt.xlabel(\"Time step\")\n",
    "#         plt.ylabel(\"Amplitude\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# plot_masked_examples(windows, masked_windows, num_examples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCMAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(1, 128, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(128, 256, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(256, 512, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv1d(512, 1024, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv1d(1024, 2048, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose1d(2048, 2048, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose1d(2048, 1024, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose1d(1024, 512, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose1d(512, 256, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose1d(256, 128, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "  )\n",
       "  (output_layer): Conv1d(128, 1, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define batch size\n",
    "batch_size = batch_size\n",
    "\n",
    "# Convert to tensors (still on CPU at this point)\n",
    "masked_X_train_tensor = torch.tensor(masked_X_train, dtype=torch.float32).unsqueeze(1)\n",
    "unmasked_X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "masked_X_test_tensor = torch.tensor(masked_X_test, dtype=torch.float32).unsqueeze(1)\n",
    "unmasked_X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets with both masked and unmasked tensors\n",
    "train_dataset = TensorDataset(masked_X_train_tensor, unmasked_X_train_tensor)\n",
    "test_dataset = TensorDataset(masked_X_test_tensor, unmasked_X_test_tensor)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = FCMAE(in_channels=1, base_dim=base_dim, num_blocks=num_blocks, kernel_size=kernel_size)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.1404\n",
      "Test Loss: 0.0612\n",
      "Epoch [2/10], Train Loss: 0.0528\n",
      "Test Loss: 0.0461\n",
      "Epoch [3/10], Train Loss: 0.0458\n",
      "Test Loss: 0.0350\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists to store train and test losses for plotting\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for masked_input, original_input in train_loader:\n",
    "        # Send data to device (GPU/CPU)\n",
    "        masked_input = masked_input.to(device)\n",
    "        original_input = original_input.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed = model(masked_input)\n",
    "\n",
    "        # Compute loss (compare reconstructed to the original unmasked input)\n",
    "        loss = criterion(reconstructed, original_input)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average train loss for this epoch\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for masked_input, original_input in test_loader:\n",
    "            # Send data to device (GPU/CPU)\n",
    "            masked_input = masked_input.to(device)\n",
    "            original_input = original_input.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed = model(masked_input)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(reconstructed, original_input)\n",
    "\n",
    "            # Update test loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Calculate average test loss for this epoch\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Plotting after training\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\", marker='x')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Test Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set the model to evaluation mode (no gradient tracking)\n",
    "# model.eval()\n",
    "\n",
    "# # Choose a test sample (here, we take the first sample from the test_loader)\n",
    "# with torch.no_grad():  # Disable gradient calculation for inference\n",
    "#     # Get a batch from the test loader\n",
    "#     for batch in test_loader:\n",
    "#         batch_input = batch[0].to(device)  # Move batch to the device (GPU or CPU)\n",
    "        \n",
    "#         # Get the reconstruction from the model\n",
    "#         output = model(batch_input)\n",
    "\n",
    "#         # Take the first test sample (index 0)\n",
    "#         original_sequence = batch_input[0].cpu().numpy().flatten()  # Flatten for easier plotting\n",
    "#         reconstructed_sequence = output[0].cpu().numpy().flatten()\n",
    "\n",
    "#         # Plotting the original and reconstructed sequence\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.plot(original_sequence, label=\"Original Sequence\")\n",
    "#         plt.plot(reconstructed_sequence, label=\"Reconstructed Sequence\", linestyle='--')\n",
    "#         plt.legend()\n",
    "#         plt.title(\"Original vs Reconstructed Sequence (Test Sample)\")\n",
    "#         plt.xlabel(\"Time Step\")\n",
    "#         plt.ylabel(\"Amplitude\")\n",
    "#         plt.show()\n",
    "\n",
    "#         break  # Only plot for the first test batch (you can modify this if you want to loop through more samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where you want to save the encoder\n",
    "# path = os.path.join(models_path, 'FCMAE_encoder_cpu_5_epochs_mr_075.pth')\n",
    "\n",
    "# # Save the encoder part of the model\n",
    "# torch.save(model.state_dict(), path)\n",
    "\n",
    "# print(f\"Model encoder saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
