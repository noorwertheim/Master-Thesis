{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.models import InceptionTime, TST, ResNet\n",
    "# from tsai.data.core import get_UCR_data\n",
    "# from tsai.learner import TSClassifier\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from CNN_classifier_model import CNNClassifier, train_model, evaluate_model\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path\n",
    "from preprocessing_modules import create_time_windows_with_labels\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import wandb\n",
    "wandb.login(key=\"5f15eb7efc1e0e939ccc83345338a0b8c24e2fbc\")\n",
    "from fastai.optimizer import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MY DATA'''\n",
    "train_dir = os.path.join(univariate_data_path, \"target_univariate_no_PCA_train_2.npy\")\n",
    "train_data = np.load(train_dir, allow_pickle=True)\n",
    "# train_data = pd.DataFrame(train_data)\n",
    "test_dir = os.path.join(univariate_data_path, \"target_univariate_no_PCA_test_2.npy\")\n",
    "test_data = np.load(test_dir, allow_pickle=True)\n",
    "# test_data = pd.DataFrame(test_data)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train_data[0])\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Example dataset with windows and labels\n",
    "train_windows, train_labels = create_time_windows_with_labels(train_data)\n",
    "test_windows, test_labels = create_time_windows_with_labels(test_data)\n",
    "\n",
    "# Count label distribution\n",
    "train_label_counts = Counter(train_labels)\n",
    "test_label_counts = Counter(test_labels)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Train label distribution:\")\n",
    "print(f\"  Term (0): {train_label_counts[0]}\")\n",
    "print(f\"  Preterm (1): {train_label_counts[1]}\")\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(f\"  Term (0): {test_label_counts[0]}\")\n",
    "print(f\"  Preterm (1): {test_label_counts[1]}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Convert to tensors\n",
    "train_windows_tensor = torch.tensor(train_windows, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_windows_tensor = torch.tensor(test_windows, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "train_windows_tensor = train_windows_tensor.unsqueeze(1)  # (1071, 1, 12000)\n",
    "test_windows_tensor = test_windows_tensor.unsqueeze(1)    # (899, 1, 12000)\n",
    "\n",
    "train_labels_tensor = train_labels_tensor.long()\n",
    "test_labels_tensor = test_labels_tensor.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "from tsai.models.ResNet import ResNet\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, Subset\n",
    "from sklearn.metrics import average_precision_score\n",
    "from fastai.metrics import Metric\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "\n",
    "# Create TensorDataset from your data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = TensorDataset(train_windows_tensor, train_labels_tensor)\n",
    "\n",
    "# ===== Compute class weights =====\n",
    "train_labels_np = train_labels_tensor.numpy()\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels_np), y=train_labels_np)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(len(train_windows_tensor))\n",
    "# Custom Average Precision Metric\n",
    "class AveragePrecision(Metric):\n",
    "    def __init__(self):\n",
    "        self.pred = []\n",
    "        self.target = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.pred, self.target = [], []\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        preds = learn.pred.detach().softmax(dim=-1)[:, 1]  # Get probabilities for class 1\n",
    "        targs = learn.y.detach()\n",
    "        self.pred.append(preds.cpu())\n",
    "        self.target.append(targs.cpu())\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        preds = torch.cat(self.pred).numpy()\n",
    "        targs = torch.cat(self.target).numpy()\n",
    "        return average_precision_score(targs, preds)\n",
    "\n",
    "    @property\n",
    "    def name(self): return \"avg_precision\"\n",
    "\n",
    "def objective(trial):\n",
    "    print('Trial:', trial)\n",
    "    print(f\"Starting trial {trial.number}\")\n",
    "\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-7, 1e-6)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['SGD', 'Adam'])\n",
    "\n",
    "    # Map optimizer name to fastai optimizer function\n",
    "    opt_func_map = {\n",
    "    'SGD': SGD,\n",
    "    'Adam': Adam,\n",
    "    }\n",
    "\n",
    "    opt_func = opt_func_map[optimizer_name]\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    avg_precisions = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.arange(len(dataset))):\n",
    "        train_dataset = Subset(dataset, train_idx)\n",
    "        val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "        dls = DataLoaders.from_dsets(\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            bs=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        model = ResNet(c_in=1, c_out=2).to(device)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "        learn = Learner(\n",
    "            dls, model,\n",
    "            loss_func=loss_func,\n",
    "            opt_func=opt_func,\n",
    "            metrics=AveragePrecision(),\n",
    "            cbs=[\n",
    "                EarlyStoppingCallback(monitor='valid_loss', patience=3),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        learn.fit_one_cycle(5, learning_rate)\n",
    "        ap = learn.validate()[1]  \n",
    "        avg_precisions.append(ap)\n",
    "        \n",
    "\n",
    "    return np.mean(avg_precisions)\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)\n",
    "\n",
    "\n",
    "# Save the study\n",
    "import joblib\n",
    "joblib.dump(study, \"ResNet_tune.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import optuna\n",
    "# import numpy as np\n",
    "# from tsai.models.ResNet import ResNet\n",
    "# from fastai.learner import Learner\n",
    "# from fastai.data.core import DataLoaders\n",
    "# from fastai.metrics import accuracy\n",
    "# from sklearn.model_selection import KFold\n",
    "# from torch.utils.data import TensorDataset, Subset\n",
    "# from fastai.callback.wandb import WandbCallback\n",
    "# from fastai.data.core import DataLoaders\n",
    "# from fastai.learner import Learner\n",
    "# from fastai.metrics import accuracy\n",
    "# from fastai.losses import CrossEntropyLossFlat\n",
    "# from sklearn.model_selection import KFold\n",
    "# from torch.utils.data import Subset\n",
    "# from tsai.models.ResNet import ResNet\n",
    "\n",
    "# # Create TensorDataset from your data\n",
    "# dataset = TensorDataset(train_windows_tensor, train_labels_tensor)\n",
    "\n",
    "# def objective(trial):\n",
    "#     print('Trial: ', trial)\n",
    "#     learning_rate = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     accs = []\n",
    "\n",
    "#     for train_idx, val_idx in kf.split(np.arange(len(dataset))):\n",
    "#         train_dataset = Subset(dataset, train_idx)\n",
    "#         val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "#         dls = DataLoaders.from_dsets(\n",
    "#             train_dataset,\n",
    "#             val_dataset,\n",
    "#             bs=batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=0  # for compatibility and reproducibility\n",
    "#         )\n",
    "#         model = ResNet(c_in=1, c_out=2)\n",
    "#         learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
    "\n",
    "#         learn.fit_one_cycle(5, lr_max=learning_rate)\n",
    "#         acc = learn.validate()[1].item()\n",
    "#         accs.append(acc)\n",
    "\n",
    "#     return np.mean(accs)\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=25)\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# print(study.best_trial)\n",
    "\n",
    "# # Save the study if you want\n",
    "# import joblib\n",
    "# joblib.dump(study, \"resnet_optuna_study.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
