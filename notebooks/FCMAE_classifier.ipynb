{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python3.11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "from tensorflow.keras import layers, models\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import tensorflow as tf\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved encoder...\n",
      "Epoch 1/5\n",
      "\u001b[1m8562/8562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 4ms/step - accuracy: 0.7371 - loss: 0.5459 - val_accuracy: 0.7137 - val_loss: 0.6403\n",
      "Epoch 2/5\n",
      "\u001b[1m8562/8562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4ms/step - accuracy: 0.7418 - loss: 0.5302 - val_accuracy: 0.7025 - val_loss: 0.6410\n",
      "Epoch 3/5\n",
      "\u001b[1m8562/8562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4ms/step - accuracy: 0.7422 - loss: 0.5264 - val_accuracy: 0.7116 - val_loss: 0.6353\n",
      "Epoch 4/5\n",
      "\u001b[1m8562/8562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4ms/step - accuracy: 0.7438 - loss: 0.5228 - val_accuracy: 0.7128 - val_loss: 0.6270\n",
      "Epoch 5/5\n",
      "\u001b[1m8562/8562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4ms/step - accuracy: 0.7435 - loss: 0.5204 - val_accuracy: 0.7129 - val_loss: 0.6439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14dab9513850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, encoder, input_shape, num_classes):\n",
    "        self.encoder = encoder\n",
    "        self.model = self._build_classifier(input_shape, num_classes)\n",
    "    \n",
    "    def _build_classifier(self, input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "        # Freeze encoder weights\n",
    "        self.encoder.trainable = False\n",
    "\n",
    "        # Ensure the input has the correct shape for the encoder (e.g., (500, 1) for FCMAE)\n",
    "        x = self.encoder(inputs, training=False)  # Use frozen encoder\n",
    "\n",
    "        # Flatten the output of the encoder (if necessary)\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        # Add dense layers for classification\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        x = layers.Dense(16, activation=\"relu\")(x)\n",
    "        outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        model = models.Model(inputs, outputs)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=64, epochs=5, validation_split=0.1):\n",
    "        history = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "# Load the trained encoder from MaskedAutoencoder\n",
    "encoder_path = \"../models/encoder_model.keras\"\n",
    "if os.path.exists(encoder_path):\n",
    "    # Load saved encoder\n",
    "    print(\"Loading saved encoder...\")\n",
    "    encoder = tf.keras.models.load_model(encoder_path)\n",
    "else:\n",
    "    # Train autoencoder and extract encoder\n",
    "    autoencoder = MaskedAutoencoder(input_dim=500)\n",
    "    input_layer = layers.Input(shape=(500,))\n",
    "    encoded_output = autoencoder.encoder(input_layer)\n",
    "    encoder = models.Model(input_layer, encoded_output)\n",
    "\n",
    "    # Save encoder\n",
    "    print(\"Saving trained encoder...\")\n",
    "    encoder.save(encoder_path)\n",
    "\n",
    "# Load the target dataset\n",
    "save_dir = os.path.join(univariate_data_path, \"target_univariate.npy\")\n",
    "target_data = np.load(save_dir, allow_pickle=True)\n",
    "\n",
    "# Create windows for training\n",
    "def create_windows(data, window_size=500, step=250):\n",
    "    all_windows, labels = [], []\n",
    "    for sample in data:\n",
    "        signal, label = sample[\"signal\"], sample[\"preterm\"]\n",
    "        \n",
    "        if label is None:  # Skip instances with None labels (just in case)\n",
    "            continue\n",
    "        \n",
    "        for start in range(0, len(signal) - window_size + 1, step):\n",
    "            window = signal[start : start + window_size]\n",
    "            all_windows.append(window)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(all_windows, dtype=np.float32), np.array(labels, dtype=np.int32)\n",
    "\n",
    "# Adjust input shape for FCMAE (e.g., (500, 1) for each window)\n",
    "x_train, y_train = create_windows(target_data)  # Make sure this function is called!\n",
    "\n",
    "# Reshape the data if needed for FCMAE (e.g., (500,) -> (500, 1))\n",
    "x_train = x_train.reshape((-1, 500, 1))  # Reshaping to match FCMAE input\n",
    "\n",
    "# Initialize and train the classifier\n",
    "num_classes = len(set(y_train))  # Get number of unique classes\n",
    "classifier = Classifier(encoder, input_shape=(500, 1), num_classes=num_classes)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.train(x_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
