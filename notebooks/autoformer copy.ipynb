{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoformerConfig, AutoformerModel\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(univariate_data_path, 'merged_univariate.npy')\n",
    "data = np.load(data_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sequence: 1793 samples (1.49 minutes)\n",
      "Longest sequence: 1002000 samples (835.00 minutes)\n",
      "Average length: 170784 samples (142.32 minutes)\n",
      "Number of sequences shorter than 10 minutes: 7\n",
      "Total number of sequences: 666\n",
      "Percentage too short: 1.05%\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(record['signal']) for record in data]\n",
    "\n",
    "# Compute and print stats\n",
    "min_len = np.min(lengths)\n",
    "max_len = np.max(lengths)\n",
    "avg_len = np.mean(lengths)\n",
    "\n",
    "print(f\"Shortest sequence: {min_len} samples ({min_len / 20 / 60:.2f} minutes)\")\n",
    "print(f\"Longest sequence: {max_len} samples ({max_len / 20 / 60:.2f} minutes)\")\n",
    "print(f\"Average length: {avg_len:.0f} samples ({avg_len / 20 / 60:.2f} minutes)\")\n",
    "\n",
    "# Count how many sequences are shorter than 10 minutes (10 * 60 * 20 = 12,000 samples)\n",
    "too_short = sum(len(record['signal']) < 12000 for record in data)\n",
    "\n",
    "print(f\"Number of sequences shorter than 10 minutes: {too_short}\")\n",
    "print(f\"Total number of sequences: {len(data)}\")\n",
    "print(f\"Percentage too short: {100 * too_short / len(data):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 659 (out of 666)\n"
     ]
    }
   ],
   "source": [
    "# Keep only signals that are >= 10 minutes (12000 samples at 20Hz)\n",
    "filtered_data = [record for record in data if len(record['signal']) >= 12000]\n",
    "\n",
    "print(f\"Filtered dataset size: {len(filtered_data)} (out of {len(data)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, temp_data = train_test_split(filtered_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12000, 1])\n",
      "torch.Size([16, 1200, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create a Forecasting Dataset\n",
    "\n",
    "class ForecastingDataset(Dataset):\n",
    "    def __init__(self, data, input_window=12000, forecast_horizon=1200, stride=6000):\n",
    "        self.samples = []\n",
    "        for record in data:\n",
    "            signal = record['signal']\n",
    "            total_len = input_window + forecast_horizon\n",
    "            for start in range(0, len(signal) - total_len + 1, stride):\n",
    "                input_seq = signal[start:start + input_window]\n",
    "                target_seq = signal[start + input_window:start + total_len]\n",
    "                self.samples.append((input_seq, target_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(input_seq, dtype=torch.float32).unsqueeze(-1),\n",
    "            torch.tensor(target_seq, dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "train_dataset = ForecastingDataset(\n",
    "    data=train_data,\n",
    "    input_window=12000,\n",
    "    forecast_horizon=1200,\n",
    "    stride=6000\n",
    ")\n",
    "\n",
    "val_dataset = ForecastingDataset(\n",
    "    data=val_data,\n",
    "    input_window=12000,\n",
    "    forecast_horizon=1200,\n",
    "    stride=6000\n",
    ")\n",
    "\n",
    "test_dataset = ForecastingDataset(\n",
    "    data=test_data,\n",
    "    input_window=12000,\n",
    "    forecast_horizon=1200,\n",
    "    stride=6000\n",
    ")\n",
    "\n",
    "# Step 3: Create DataLoaders for Each Dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Example of fetching a batch\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape)  # (batch_size, input_window, 1)\n",
    "print(y.shape)  # (batch_size, forecast_horizon, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.autoformer.modeling_autoformer.AutoformerModel'>\n",
      "AutoformerModel(\n",
      "  (scaler): AutoformerMeanScaler()\n",
      "  (encoder): AutoformerEncoder(\n",
      "    (value_embedding): AutoformerValueEmbedding(\n",
      "      (value_projection): Linear(in_features=9, out_features=256, bias=False)\n",
      "    )\n",
      "    (embed_positions): AutoformerSinusoidalPositionalEmbedding(2400, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x AutoformerEncoderLayer(\n",
      "        (self_attn): AutoformerAttention(\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=256, bias=True)\n",
      "        (final_layer_norm): AutoformerLayernorm(\n",
      "          (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (decomp1): AutoformerSeriesDecompositionLayer(\n",
      "          (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "        )\n",
      "        (decomp2): AutoformerSeriesDecompositionLayer(\n",
      "          (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): AutoformerDecoder(\n",
      "    (value_embedding): AutoformerValueEmbedding(\n",
      "      (value_projection): Linear(in_features=9, out_features=256, bias=False)\n",
      "    )\n",
      "    (embed_positions): AutoformerSinusoidalPositionalEmbedding(2400, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x AutoformerDecoderLayer(\n",
      "        (self_attn): AutoformerAttention(\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): AutoformerAttention(\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=256, bias=True)\n",
      "        (final_layer_norm): AutoformerLayernorm(\n",
      "          (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (decomp1): AutoformerSeriesDecompositionLayer(\n",
      "          (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "        )\n",
      "        (decomp2): AutoformerSeriesDecompositionLayer(\n",
      "          (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "        )\n",
      "        (decomp3): AutoformerSeriesDecompositionLayer(\n",
      "          (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "        )\n",
      "        (trend_projection): Conv1d(256, 9, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (seasonality_projection): Linear(in_features=256, out_features=9, bias=True)\n",
      "  )\n",
      "  (decomposition_layer): AutoformerSeriesDecompositionLayer(\n",
      "    (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize AutoformerConfig & Autoformer model\n",
    "\n",
    "config = AutoformerConfig(\n",
    "    prediction_length=1200,   \n",
    "    hidden_size=256,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=3,\n",
    "    intermediate_size=512,\n",
    "    layer_norm_eps=1e-12,\n",
    "    max_position_embeddings=12000,\n",
    "    num_labels=1,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    num_static_real_features=0,\n",
    "    num_static_categorical_features=0   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = AutoformerModel(config)\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a Forecasting Head\n",
    "\n",
    "# class AutoformerForForecasting(nn.Module):\n",
    "#     def __init__(self, model, output_size):\n",
    "#         super(AutoformerForForecasting, self).__init__()\n",
    "#         self.autoformer = model\n",
    "#         self.output_layer = nn.Linear(self.autoformer.config.hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, past_values, past_time_features=None, past_observed_mask=None, static_real_features=None, static_categorical_features=None):\n",
    "#         print(f\"past_values.shape: {past_values.shape}\")\n",
    "#         print(f\"past_time_features.shape: {past_time_features.shape if past_time_features is not None else 'None'}\")\n",
    "#         print(f\"past_observed_mask.shape: {past_observed_mask.shape if past_observed_mask is not None else 'None'}\")\n",
    "#         if static_real_features is not None:\n",
    "#             print(f\"static_real_features.shape: {static_real_features.shape}\")\n",
    "#         if static_categorical_features is not None:\n",
    "#             print(f\"static_categorical_features.shape: {static_categorical_features.shape}\")\n",
    "        \n",
    "#         if self.autoformer.config.num_static_real_features == 0:\n",
    "#             static_real_features = None\n",
    "#         if self.autoformer.config.num_static_categorical_features == 0:\n",
    "#             static_categorical_features = None\n",
    "\n",
    "#         # Pass through the Autoformer model (only past_values if it's the required input)\n",
    "#         encoder_output = self.autoformer(past_values)[0]  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "#         # Use the last hidden state for forecasting\n",
    "#         forecast = self.output_layer(encoder_output[:, -1, :])  # (batch_size, forecast_horizon)\n",
    "#         return forecast\n",
    "\n",
    "\n",
    "class AutoformerForForecasting(nn.Module):\n",
    "    def __init__(self, model, output_size):\n",
    "        super(AutoformerForForecasting, self).__init__()\n",
    "        self.autoformer = model\n",
    "        self.output_layer = nn.Linear(config.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, past_values, past_time_features, past_observed_mask, static_real_features=None, static_categorical_features=None):\n",
    "        # Forward pass through the Autoformer model\n",
    "        encoder_output = self.autoformer(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            static_real_features=static_real_features,\n",
    "            static_categorical_features=static_categorical_features\n",
    "        )[0]  # Output from the encoder (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Use the last hidden state to make the forecast\n",
    "        forecast = self.output_layer(encoder_output[:, -1, :])  # Output from the last time step\n",
    "        return forecast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Loss Function & Optimizer\n",
    "# # Use MSE and ADAM-optimizer\n",
    "\n",
    "# # Loss function (Mean Squared Error)\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# # Optimizer (Adam optimizer with learning rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.autoformer.modeling_autoformer.AutoformerModel'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[16, 1, 2, 1]}, size=[-1, 11993, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Train the model on the forecasting task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mtrain_autoformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain_autoformer\u001b[39m\u001b[34m(model, dataloader, epochs, learning_rate)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Pass data through the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m forecast = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can adjust the time features if needed\u001b[39;49;00m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Modify this as necessary\u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m     66\u001b[39m loss = mse_loss(forecast, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master-Thesis/thesisenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master-Thesis/thesisenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master-Thesis/thesisenv/lib/python3.11/site-packages/transformers/models/autoformer/modeling_autoformer.py:1657\u001b[39m, in \u001b[36mAutoformerModel.forward\u001b[39m\u001b[34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[39m\n\u001b[32m   1654\u001b[39m use_cache = use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_cache\n\u001b[32m   1655\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m transformer_inputs, temporal_features, loc, scale, static_feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1668\u001b[39m     enc_input = torch.cat(\n\u001b[32m   1669\u001b[39m         (\n\u001b[32m   1670\u001b[39m             transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m.config.context_length, ...],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1673\u001b[39m         dim=-\u001b[32m1\u001b[39m,\n\u001b[32m   1674\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Master-Thesis/thesisenv/lib/python3.11/site-packages/transformers/models/autoformer/modeling_autoformer.py:1571\u001b[39m, in \u001b[36mAutoformerModel.create_network_inputs\u001b[39m\u001b[34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[39m\n\u001b[32m   1569\u001b[39m     embedded_cat = \u001b[38;5;28mself\u001b[39m.embedder(static_categorical_features)\n\u001b[32m   1570\u001b[39m     static_feat = torch.cat((embedded_cat, static_feat), dim=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m expanded_static_feat = \u001b[43mstatic_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[38;5;66;03m# all features\u001b[39;00m\n\u001b[32m   1574\u001b[39m features = torch.cat((expanded_static_feat, time_feat), dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: expand(torch.FloatTensor{[16, 1, 2, 1]}, size=[-1, 11993, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "# # Training Loop\n",
    "\n",
    "# def train(model, dataloader, epochs):\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         epoch_loss = 0\n",
    "#         for batch_idx, (x, y) in enumerate(dataloader):\n",
    "#             batch_size = x.size(0)\n",
    "#             print(f\"x.shape: {x.shape}\")\n",
    "#             print(f\"y.shape: {y.shape}\")\n",
    "\n",
    "#             # Dummy time features and mask\n",
    "#             past_time_features = torch.zeros_like(x).to(x.device)\n",
    "#             past_observed_mask = torch.ones_like(x).to(x.device)\n",
    "\n",
    "#             # print(f\"x.shape: {x.shape}\")\n",
    "#             # print(f\"past_time_features.shape: {past_time_features.shape}\")\n",
    "#             # print(f\"past_observed_mask.shape: {past_observed_mask.shape}\")\n",
    "#             print(f\"Model type: {type(model)}\")\n",
    "\n",
    "#             output = model(\n",
    "#                 past_values=x,\n",
    "#                 past_time_features=past_time_features,\n",
    "#                 past_observed_mask=past_observed_mask\n",
    "#             )\n",
    "#             forecast = output.prediction\n",
    "\n",
    "#             loss = loss_fn(forecast, y)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# # Train the model\n",
    "# train(model, train_dataloader, epochs=10)\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Loss function\n",
    "def mse_loss(pred, true):\n",
    "    return F.mse_loss(pred, true)\n",
    "\n",
    "# Training Loop\n",
    "def train_autoformer(model, dataloader, epochs=10, learning_rate=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Model type: {type(model)}\")\n",
    "            # Pass data through the model\n",
    "            forecast = model(\n",
    "                past_values=x, \n",
    "                past_time_features=x,  # You can adjust the time features if needed\n",
    "                past_observed_mask=torch.ones_like(x)  # Modify this as necessary\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = mse_loss(forecast, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "\n",
    "# Train the model on the forecasting task\n",
    "train_autoformer(model, train_dataloader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = os.path.join(models_path, \"autoformer_forecasting_model.pth\")\n",
    "torch.save(model.state_dict(), save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
