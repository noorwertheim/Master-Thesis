{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import AutoformerConfig, AutoformerForPrediction\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoformerConfig, AutoformerForPrediction\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(univariate_data_path, 'merged_univariate.npy')\n",
    "data = np.load(data_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 659 (out of 666)\n",
      "{'record_name': 'ice008_p_4of4', 'signal': array([ 40.91904   ,  40.44264156,  39.96763369, ..., -13.16801657,\n",
      "       -13.16801289, -13.16801   ]), 'metadata': {'fs': 200, 'sig_len': 786000, 'n_sig': 16, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV', 'mV'], 'comments': ['Info:', 'ID:ice008', 'Record type:pregnancy', 'Record number:4/4', 'Age(years):26', 'BMI before pregnancy:20.9', 'BMI at recording:25.3', 'Gravidity:5', 'Parity:2', 'Previous caesarean:No', 'Placental position:Anterior', 'Gestational age at recording(w/d):40/1', 'Gestational age at delivery:40/2', 'Mode of delivery:Vaginal', 'Synthetic oxytocin use in labour:No', 'Epidural during labour:No', 'Comments for recording:', 'A contraction had just started at the beginning of this recording.', 'Baby was born 17 hours and 20 minutes after the end of this recording.', 'Comments for delivery:']}}\n"
     ]
    }
   ],
   "source": [
    "# Keep only signals that are >= 10 minutes (12000 samples at 20Hz)\n",
    "filtered_data = [record for record in data if len(record['signal']) >= 12000]\n",
    "\n",
    "print(f\"Filtered dataset size: {len(filtered_data)} (out of {len(data)})\")\n",
    "train_data, temp_data = train_test_split(filtered_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(train_data, val_data, test_data, batch_size=16, window_size=12000, prediction_length=1200, stride=400):\n",
    "    \"\"\"Prepare DataLoaders for training, validation and testing\"\"\"\n",
    "    # Extract signal data from the dictionaries\n",
    "    train_signals = [item['signal'] for item in train_data]\n",
    "    val_signals = [item['signal'] for item in val_data]\n",
    "    test_signals = [item['signal'] for item in test_data]\n",
    "    \n",
    "    print(f\"Number of training signals: {len(train_signals)}\")\n",
    "    print(f\"Sample signal length: {len(train_signals[0])}\")\n",
    "    \n",
    "    # Concatenate all signals into single arrays for each split\n",
    "    # Another approach would be to create separate datasets for each signal\n",
    "    train_signal_concat = np.concatenate(train_signals)\n",
    "    val_signal_concat = np.concatenate(val_signals)\n",
    "    test_signal_concat = np.concatenate(test_signals)\n",
    "    \n",
    "    print(f\"Concatenated training signal length: {len(train_signal_concat)}\")\n",
    "    \n",
    "    # Create datasets with the extracted signals\n",
    "    train_dataset = TimeSeriesDataset(train_signal_concat, window_size, prediction_length, stride)\n",
    "    val_dataset = TimeSeriesDataset(val_signal_concat, window_size, prediction_length, stride)\n",
    "    test_dataset = TimeSeriesDataset(test_signal_concat, window_size, prediction_length, stride)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for loading time series data with sliding windows for forecasting.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, window_size=12000, prediction_length=1200, stride=400):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Time series data (numpy array)\n",
    "            window_size: Size of input context window (in data points)\n",
    "            prediction_length: Number of future points to predict\n",
    "            stride: Step size between windows\n",
    "        \"\"\"\n",
    "        # Ensure data is a numpy array\n",
    "        self.data = data if isinstance(data, np.ndarray) else np.array(data, dtype=np.float32)\n",
    "            \n",
    "        # Ensure data is 1D\n",
    "        if len(self.data.shape) > 1:\n",
    "            self.data = self.data.flatten()\n",
    "            \n",
    "        self.window_size = window_size\n",
    "        self.prediction_length = prediction_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(self.data) < window_size + prediction_length:\n",
    "            raise ValueError(f\"Not enough data points. Have {len(self.data)}, need at least {window_size + prediction_length}\")\n",
    "        \n",
    "        # Create indices for sliding windows\n",
    "        self.indices = []\n",
    "        for i in range(0, len(self.data) - window_size - prediction_length + 1, stride):\n",
    "            self.indices.append(i)\n",
    "            \n",
    "        if len(self.indices) == 0:\n",
    "            raise ValueError(\"No valid windows could be created with the current parameters\")\n",
    "            \n",
    "        # Standardize the data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_data = self.scaler.fit_transform(self.data.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        print(f\"Created dataset with {len(self.indices)} windows\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        end_idx = start_idx + self.window_size\n",
    "        future_idx = end_idx + self.prediction_length\n",
    "        \n",
    "        # Get context window and prediction target\n",
    "        context = self.scaled_data[start_idx:end_idx]\n",
    "        target = self.scaled_data[end_idx:future_idx]\n",
    "        \n",
    "        return {\n",
    "            'context': torch.FloatTensor(context),\n",
    "            'target': torch.FloatTensor(target)\n",
    "        }\n",
    "    \n",
    "    def inverse_transform(self, scaled_data):\n",
    "        \"\"\"Convert scaled data back to original scale\"\"\"\n",
    "        if isinstance(scaled_data, torch.Tensor):\n",
    "            scaled_data = scaled_data.numpy()\n",
    "        return self.scaler.inverse_transform(scaled_data.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 170\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_test_loss, all_predictions, all_targets\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[43mmain\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle batches of time series\"\"\"\n",
    "    contexts = torch.stack([item['context'] for item in batch])\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    \n",
    "    # Reshape for Autoformer (batch_size, seq_len, features)\n",
    "    contexts = contexts.unsqueeze(-1)\n",
    "    targets = targets.unsqueeze(-1)\n",
    "    \n",
    "    return {\n",
    "        'context': contexts,\n",
    "        'target': targets\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10, learning_rate=1e-4):\n",
    "    \"\"\"Train the Autoformer model\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = 'best_autoformer_model.pth'\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training step\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            context = batch['context'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            # Create observed mask (all True since we're assuming all past values are observed)\n",
    "            past_observed_mask = torch.ones_like(context, dtype=torch.bool).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with past_observed_mask\n",
    "            outputs = model(\n",
    "                past_values=context,\n",
    "                past_observed_mask=past_observed_mask,  # Add this parameter\n",
    "                past_time_features=None,  # We're not using explicit time features here\n",
    "                future_values=None,  # During training, we don't provide future values\n",
    "                future_time_features=None,\n",
    "            )\n",
    "            \n",
    "            # Extract predictions\n",
    "            predictions = outputs.prediction_outputs\n",
    "            \n",
    "            # Calculate loss (MSE)\n",
    "            loss = criterion(predictions, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}\")\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                context = batch['context'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                \n",
    "                # Create observed mask for validation\n",
    "                past_observed_mask = torch.ones_like(context, dtype=torch.bool).to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    past_values=context,\n",
    "                    past_observed_mask=past_observed_mask,  # Add this parameter\n",
    "                    past_time_features=None,\n",
    "                    future_values=None,\n",
    "                    future_time_features=None,\n",
    "                )\n",
    "                \n",
    "                predictions = outputs.prediction_outputs\n",
    "                loss = criterion(predictions, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {avg_val_loss:.6f}, Time: {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            logger.info(f\"Saved best model with validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, test_dataset, device):\n",
    "    \"\"\"Evaluate the trained model on test data\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            context = batch['context'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            # Create observed mask for testing\n",
    "            past_observed_mask = torch.ones_like(context, dtype=torch.bool).to(device)\n",
    "            print('type model: ', model.type)\n",
    "            outputs = model(\n",
    "                past_values=context,\n",
    "                past_observed_mask=past_observed_mask,  # Add this parameter\n",
    "                past_time_features=None,\n",
    "                future_values=None,\n",
    "                future_time_features=None,\n",
    "            )\n",
    "            \n",
    "            predictions = outputs.prediction_outputs\n",
    "            loss = criterion(predictions, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and targets for visualization\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    logger.info(f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    # Convert predictions and targets back to original scale\n",
    "    all_predictions = np.concatenate([p.reshape(-1, 1) for p in all_predictions], axis=0)\n",
    "    all_targets = np.concatenate([t.reshape(-1, 1) for t in all_targets], axis=0)\n",
    "    \n",
    "    # Visualize some predictions\n",
    "    sample_idx = np.random.randint(0, len(all_predictions), 3)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        pred = test_dataset.inverse_transform(all_predictions[idx].flatten())\n",
    "        true = test_dataset.inverse_transform(all_targets[idx].flatten())\n",
    "        \n",
    "        plt.subplot(3, 1, i+1)\n",
    "        plt.plot(true, label='Ground Truth', color='blue')\n",
    "        plt.plot(pred, label='Prediction', color='red', linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.title(f'Sample {idx}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('forecast_predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return avg_test_loss, all_predictions, all_targets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
