{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwertheim/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoformerConfig, AutoformerModel\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path, models_path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoformerConfig, AutoformerForPrediction\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(univariate_data_path, 'merged_univariate.npy')\n",
    "data = np.load(data_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sequence: 17 samples (0.01 minutes)\n",
      "Longest sequence: 100200 samples (83.50 minutes)\n",
      "Average length: 35766 samples (29.80 minutes)\n",
      "[[-1.7358303 ]\n",
      " [-0.30347557]\n",
      " [-0.40749874]\n",
      " ...\n",
      " [-3.09738299]\n",
      " [-2.90981482]\n",
      " [-3.22768386]]\n",
      "Number of sequences shorter than 10 minutes: 83\n",
      "Total number of sequences: 666\n",
      "Percentage too short: 12.46%\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(record['signal']) for record in data]\n",
    "\n",
    "# Compute and print stats\n",
    "min_len = np.min(lengths)\n",
    "max_len = np.max(lengths)\n",
    "avg_len = np.mean(lengths)\n",
    "\n",
    "print(f\"Shortest sequence: {min_len} samples ({min_len / 20 / 60:.2f} minutes)\")\n",
    "print(f\"Longest sequence: {max_len} samples ({max_len / 20 / 60:.2f} minutes)\")\n",
    "print(f\"Average length: {avg_len:.0f} samples ({avg_len / 20 / 60:.2f} minutes)\")\n",
    "\n",
    "# Count how many sequences are shorter than 10 minutes (10 * 60 * 20 = 12,000 samples)\n",
    "too_short = sum(len(record['signal']) < 12000 for record in data)\n",
    "print(data[0]['signal'])\n",
    "print(f\"Number of sequences shorter than 10 minutes: {too_short}\")\n",
    "print(f\"Total number of sequences: {len(data)}\")\n",
    "print(f\"Percentage too short: {100 * too_short / len(data):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 583 (out of 666)\n"
     ]
    }
   ],
   "source": [
    "# Keep only signals that are >= 10 minutes (12000 samples at 20Hz)\n",
    "filtered_data = [record for record in data if len(record['signal']) >= 12000]\n",
    "\n",
    "print(f\"Filtered dataset size: {len(filtered_data)} (out of {len(data)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, temp_data = train_test_split(filtered_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12000, 1])\n",
      "torch.Size([16, 1200, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create a Forecasting Dataset\n",
    "class ForecastingDataset(Dataset):\n",
    "    def __init__(self, data, input_window=12000, forecast_horizon=1200, stride=6000):\n",
    "        self.samples = []\n",
    "        for record in data:\n",
    "            signal = record['signal']\n",
    "            total_len = input_window + forecast_horizon\n",
    "            for start in range(0, len(signal) - total_len + 1, stride):\n",
    "                input_seq = signal[start:start + input_window]\n",
    "                target_seq = signal[start + input_window:start + total_len]\n",
    "                self.samples.append((input_seq, target_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.samples[idx]\n",
    "\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "        # Ensure tensors are 3D: (seq_len, num_features)\n",
    "        if input_tensor.ndim == 1:\n",
    "            input_tensor = input_tensor.unsqueeze(-1)\n",
    "        if target_tensor.ndim == 1:\n",
    "            target_tensor = target_tensor.unsqueeze(-1)\n",
    "\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "train_dataset = ForecastingDataset(\n",
    "    data=train_data,\n",
    "    input_window=12000,\n",
    "    forecast_horizon=1200,\n",
    "    stride=6000\n",
    ")\n",
    "\n",
    "val_dataset = ForecastingDataset(\n",
    "    data=val_data,\n",
    "    input_window=12000,\n",
    "    forecast_horizon=1200,\n",
    "    stride=6000\n",
    ")\n",
    "\n",
    "test_dataset = ForecastingDataset(\n",
    "    data=test_data,\n",
    "    input_window=12000,\n",
    "    forecast_horizon=1200,\n",
    "    stride=6000\n",
    ")\n",
    "\n",
    "# Step 3: Create DataLoaders for Each Dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Example of fetching a batch\n",
    "x, y = next(iter(train_dataloader))\n",
    "\n",
    "print(x.shape)  # (batch_size, input_window, 1)\n",
    "print(y.shape)  # (batch_size, forecast_horizon, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AutoformerConfig & Autoformer model\n",
    "\n",
    "from transformers import AutoformerConfig\n",
    "\n",
    "# Define the configuration for the Autoformer model\n",
    "config = AutoformerConfig(\n",
    "    input_size=1,\n",
    "    input_length=12000,\n",
    "    output_length=1200,\n",
    "    prediction_length=1200,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=8,\n",
    "    d_model=64,\n",
    "    dropout_rate=0.1, \n",
    ")\n",
    "\n",
    "    # num_static_categorical_features=0,\n",
    "    # num_static_real_features=1, \n",
    "from transformers import AutoformerForPrediction\n",
    "\n",
    "# Initialize the model with the configuration\n",
    "model = AutoformerForPrediction(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class AutoformerForForecasting(nn.Module):\n",
    "#     def __init__(self, model, output_size):\n",
    "#         super(AutoformerForForecasting, self).__init__()\n",
    "#         self.autoformer = model\n",
    "#         self.output_layer = nn.Linear(self.autoformer.config.hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, past_values):\n",
    "#         \"\"\"\n",
    "#         Forward pass through Autoformer.\n",
    "\n",
    "#         Parameters:\n",
    "#         - past_values: (batch_size, seq_len, input_size) Time series data\n",
    "\n",
    "#         Returns:\n",
    "#         - forecast: (batch_size, output_size) Forecasted values\n",
    "#         \"\"\"\n",
    "#         if past_time_features is not None and past_observed_mask is not None:\n",
    "#             # Handle the time features and observed mask here\n",
    "#             # For example, add them to your input processing layers or combine them with past_values\n",
    "#             pass\n",
    "#         # Print the input shape for debugging\n",
    "#         print(f\"past_values.shape: {past_values.shape}\")\n",
    "        \n",
    "#         # Pass the past_values through the Autoformer model\n",
    "#         encoder_output = self.autoformer(past_values)[0]  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "#         # Use the last hidden state for forecasting (last time step)\n",
    "#         forecast = self.output_layer(encoder_output[:, -1, :])  # (batch_size, output_size)\n",
    "#         return forecast\n",
    "\n",
    "# model = AutoformerForForecasting(base_model, output_size=1200)  # Output size is the forecast horizon\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([16, 12000, 1])\n",
      "y.shape: torch.Size([16, 1200, 1])\n",
      "past_values: torch.Size([16, 12000, 1])\n",
      "past_time_features: torch.Size([16, 12000, 1])\n",
      "past_observed_mask: torch.Size([16, 12000, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[16, 1, 2, 1]}, size=[-1, 11993, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Define the training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, epochs, loss_fn, optimizer, device)\u001b[39m\n\u001b[32m     41\u001b[39m empty_static_real = torch.empty(x.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, dtype=torch.float32, device=x.device)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Trace the part of the model where the error occurs and inspect the expansion logic.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# If needed, inspect the `expand` and `unsqueeze` operations inside the model to make sure they are applied correctly.\u001b[39;00m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Forward pass: Model should forecast the next 1200 points (forecast_horizon)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m forecast = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Ensure forecast size is (batch_size, forecast_horizon, 1) and matches y\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m forecast.shape == y.shape, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForecast shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, y shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/autoformer/modeling_autoformer.py:1915\u001b[39m, in \u001b[36mAutoformerForPrediction.forward\u001b[39m\u001b[34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, future_observed_mask, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m future_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1913\u001b[39m     use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1935\u001b[39m prediction_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1936\u001b[39m params = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/autoformer/modeling_autoformer.py:1657\u001b[39m, in \u001b[36mAutoformerModel.forward\u001b[39m\u001b[34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[39m\n\u001b[32m   1654\u001b[39m use_cache = use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_cache\n\u001b[32m   1655\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m transformer_inputs, temporal_features, loc, scale, static_feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1668\u001b[39m     enc_input = torch.cat(\n\u001b[32m   1669\u001b[39m         (\n\u001b[32m   1670\u001b[39m             transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m.config.context_length, ...],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1673\u001b[39m         dim=-\u001b[32m1\u001b[39m,\n\u001b[32m   1674\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/autoformer/modeling_autoformer.py:1571\u001b[39m, in \u001b[36mAutoformerModel.create_network_inputs\u001b[39m\u001b[34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[39m\n\u001b[32m   1569\u001b[39m     embedded_cat = \u001b[38;5;28mself\u001b[39m.embedder(static_categorical_features)\n\u001b[32m   1570\u001b[39m     static_feat = torch.cat((embedded_cat, static_feat), dim=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m expanded_static_feat = \u001b[43mstatic_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[38;5;66;03m# all features\u001b[39;00m\n\u001b[32m   1574\u001b[39m features = torch.cat((expanded_static_feat, time_feat), dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: expand(torch.FloatTensor{[16, 1, 2, 1]}, size=[-1, 11993, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, dataloader, epochs, loss_fn, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    model.to(device)  # Move the model to the GPU or CPU\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)  # Move data to the same device as the model\n",
    "            \n",
    "            batch_size = x.size(0)\n",
    "            sequence_length = x.size(1)\n",
    "           \n",
    "\n",
    "            print(f\"x.shape: {x.shape}\")  # Should be (batch_size, input_window, 1)\n",
    "            print(f\"y.shape: {y.shape}\")  # Should be (batch_size, forecast_horizon, 1)\n",
    "\n",
    "            # Create dummy past_time_features with shape (batch_size, input_window, 1)\n",
    "            # Ensure the input dimensions are correct and consistent\n",
    "            past_time_features = torch.zeros(batch_size, sequence_length, 1, device=device)  # Dummy tensor with the same shape as x\n",
    "            # past_observed_mask shape should be [batch_size, sequence_length, 1]\n",
    "            # past_observed_mask = torch.ones(batch_size, sequence_length, 1, dtype=torch.bool, device=device)  # Shape: [16, 12000, 1]\n",
    "            past_observed_mask = torch.ones_like(x).int()  # Shape: [16, 12000, 1]\n",
    "\n",
    "            # past_time_features = past_time_features.squeeze(-1)  # Remove the singleton dimension\n",
    "            # past_observed_mask = past_observed_mask.squeeze(-1)  # Remove the singleton dimension\n",
    "            # past_time_features = \n",
    "\n",
    "            # Print shape of tensors at the point of error\n",
    "            print(\"past_values:\", x.shape)\n",
    "            print(\"past_time_features:\", past_time_features.shape)\n",
    "            print(\"past_observed_mask:\", past_observed_mask.shape)\n",
    "            batch_size = x.shape[0]\n",
    "            device = x.device\n",
    "            empty_static_cat = torch.empty(x.shape[0], 1, 0, dtype=torch.long, device=x.device)  # Empty categorical features (with batch size)\n",
    "            empty_static_real = torch.empty(x.shape[0], 1, 1, dtype=torch.float32, device=x.device)\n",
    "            # Trace the part of the model where the error occurs and inspect the expansion logic.\n",
    "            # If needed, inspect the `expand` and `unsqueeze` operations inside the model to make sure they are applied correctly.\n",
    "\n",
    "            # Forward pass: Model should forecast the next 1200 points (forecast_horizon)\n",
    "            forecast = model(past_values=x, past_time_features=past_time_features, past_observed_mask=past_observed_mask)\n",
    "\n",
    "\n",
    "            # Ensure forecast size is (batch_size, forecast_horizon, 1) and matches y\n",
    "            assert forecast.shape == y.shape, f\"Forecast shape: {forecast.shape}, y shape: {y.shape}\"\n",
    "            print(\"distribution shape:\", distribution.shape)\n",
    "            print(\"future_values shape:\", future_values.shape)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(forecast, y)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Instantiate the loss function and optimizer\n",
    "loss_fn = MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the training loop\n",
    "train(model, train_dataloader, epochs=10, loss_fn=loss_fn, optimizer=optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = os.path.join(models_path, \"autoformer_forecasting_model.pth\")\n",
    "torch.save(model.state_dict(), save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
