{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes, trims, and filters NINFEADB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from scipy.signal import butter, filtfilt, sosfiltfilt, decimate\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter datasets you want to preprocess\n",
    "datasets = ['tpehgt', 'tpehgdb', 'ehgdb1', 'ehgdb2', 'icehgds', 'ninfea', 'nifeadb'] # for when running process_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_info(dataset): \n",
    "    data_path = os.path.join(raw_data_path, dataset + '_data.npy')\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "    # Print basic dataset info\n",
    "    print(f\"Total number of entries: {len(data)}\")\n",
    "    print(f\"First entry keys: {list(data[0].keys())}\")  # Check the dictionary structure\n",
    "\n",
    "    # Extract sequence lengths\n",
    "    sequence_lengths = np.array([entry['signal'].shape[0] for entry in data])\n",
    "\n",
    "    # Compute statistics\n",
    "    print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
    "    print(f\"Min sequence length: {np.min(sequence_lengths)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence lengths: {np.std(sequence_lengths):.2f}\")\n",
    "\n",
    "    # Check number of channels\n",
    "    num_channels = set(entry['signal'].shape[1] for entry in data)\n",
    "    print(f\"Unique number of channels in dataset: {num_channels}\")\n",
    "\n",
    "    # Print a sample metadata entry\n",
    "    print(\"Sample metadata:\", {k: v for k, v in data[0].items() if k != 'signal'})\n",
    "    print(data[0]['metadata']['fs'])\n",
    "    print('Number of channels: ', data[0]['signal'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot signals\n",
    "def plot_signals(signal, title='Signals', filename=None):\n",
    "    num_channels = signal.shape[1]\n",
    "    fig, axes = plt.subplots(num_channels, 1, figsize=(12, 2 * num_channels), sharex=True)\n",
    "\n",
    "    for i in range(num_channels):\n",
    "        axes[i].plot(signal[:, i], label=f'Channel {i+1}')\n",
    "        axes[i].legend(loc='upper right')\n",
    "        axes[i].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time Steps\")\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Save the figure\n",
    "    # plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    # plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trim_data(data, dataset_name, sec_to_remove=60):\n",
    "    \"\"\"\n",
    "    Trim the first and last `sec_to_remove` seconds from each sequence in the dataset.\n",
    "    Returns a new dataset with trimmed signals.\n",
    "    \"\"\"\n",
    "    trimmed_data = []\n",
    "    if dataset_name == 'ninfea':\n",
    "        sec_to_remove = 10\n",
    "    \n",
    "    for entry in data:\n",
    "        sampling_frequency = int(entry['metadata']['fs'])\n",
    "        ts_to_remove = sec_to_remove * sampling_frequency\n",
    "        \n",
    "        signal = entry['signal']\n",
    "        trimmed_signal = signal[ts_to_remove:-ts_to_remove] if 2 * ts_to_remove < len(signal) else signal\n",
    "        \n",
    "        # Remove specific channels if dataset is 'ninfea'\n",
    "        if dataset_name == 'ninfea':\n",
    "            channels_to_remove = [27, 28, 29, 30, 32, 33] # Remove channels containing 0.0 values\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "        \n",
    "        # Remove last two channels if dataset is 'tpehgt'\n",
    "        if dataset_name == 'tpehgt':\n",
    "            channels_to_remove = [1, 3, 5, 6, 7] # Remove filtered/TOCO channels (TOCO = channel 6 & 7)\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "\n",
    "        if dataset_name == 'tpehgdb':\n",
    "            channels_to_remove = [1, 2, 3, 5, 6, 7, 9, 10, 11] # Remove filtered channels\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "\n",
    "        if dataset_name == 'icehgds':\n",
    "            channels_to_remove = [1, 3, 5] # Remove filtered channels\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "\n",
    "        trimmed_data.append({\n",
    "            'record_name': entry['record_name'],\n",
    "            'signal': trimmed_signal,\n",
    "            'metadata': entry['metadata']\n",
    "        })\n",
    "    \n",
    "    return trimmed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth bandpass filter to the signal.\n",
    "    \"\"\"\n",
    "    b, a = butter(order, highcut, 'high', fs=fs)\n",
    "    y = filtfilt(b, a, data, axis=0)\n",
    "    b, a = butter(order, lowcut, 'low', fs=fs)\n",
    "    return filtfilt(b, a, y, axis=0)\n",
    "\n",
    "def filter_data(data, bandwidth=[0.3, 0.4]):\n",
    "    \"\"\"\n",
    "    Filter each channel of the signal within the specified bandwidth.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        signal = entry['signal']\n",
    "        fs = entry['metadata']['fs']\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        filtered_signal = butter_bandpass_filter(signal, lowcut=bandwidth[0], highcut=bandwidth[1], fs=fs)\n",
    "        \n",
    "        filtered_data.append({\n",
    "            'record_name': entry['record_name'],\n",
    "            'signal': filtered_signal,\n",
    "            'metadata': entry['metadata']\n",
    "        })\n",
    "    \n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def z_normalize_signals(data, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Apply z-normalization to each channel in the multivariate time series dataset.\n",
    "    \"\"\"\n",
    "    normalized_entries = []\n",
    "    \n",
    "    for entry in data:\n",
    "        signal = entry['signal']  # Shape: (sequence_length, num_channels)\n",
    "        \n",
    "        if signal.ndim == 1:\n",
    "            signal = signal[:, np.newaxis]  # Ensure 2D array for consistency\n",
    "        \n",
    "        # Compute mean and std for each channel separately\n",
    "        mu = np.mean(signal, axis=0)\n",
    "        sigma = np.std(signal, axis=0)\n",
    "        \n",
    "        # Normalize each channel\n",
    "        normalized_signal = (signal - mu) / (sigma + epsilon)\n",
    "        \n",
    "        # Store the normalized entry\n",
    "        normalized_entries.append({\n",
    "            'record_name': entry['record_name'],\n",
    "            'signal': normalized_signal\n",
    "        })\n",
    "    \n",
    "    return normalized_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_normalize(data, tol=1e-2):\n",
    "    \"\"\"\n",
    "    Check if the normalized dataset has mean ~0 and variance ~1 for each channel.\n",
    "    \"\"\"\n",
    "    all_correct = True\n",
    "    incorrect_entries = []\n",
    "    \n",
    "    for entry in data:\n",
    "        signal = entry['signal']\n",
    "        mean_per_channel = np.mean(signal, axis=0)\n",
    "        std_per_channel = np.std(signal, axis=0)\n",
    "        \n",
    "        incorrect = np.where((np.abs(mean_per_channel) >= tol) | (np.abs(std_per_channel - 1) >= tol))[0]\n",
    "        if len(incorrect) > 0:\n",
    "            all_correct = False\n",
    "            incorrect_entries.append((entry['record_name'], incorrect, mean_per_channel[incorrect], std_per_channel[incorrect]))\n",
    "    \n",
    "    if all_correct:\n",
    "        print(\"Normalization check passed: All channels have mean ≈ 0 and std ≈ 1.\")\n",
    "    else:\n",
    "        print(\"Normalization check failed: Some channels deviate from expected mean and std.\")\n",
    "        for record_name, incorrect, means, stds in incorrect_entries:\n",
    "            print(f\"Record {record_name}: \")\n",
    "            for ch, mean, std in zip(incorrect, means, stds):\n",
    "                print(f\"  Channel {ch}: mean = {mean:.4f}, std = {std:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now preprocessing  tpehgt\n",
      "signal before trimming/deleting channels: (35300, 8)\n",
      "signal after trimming/deleting channels: (32900, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpehgt saved!\n",
      "Now preprocessing  tpehgdb\n",
      "signal before trimming/deleting channels: (35180, 12)\n",
      "signal after trimming/deleting channels: (32780, 3)\n",
      "tpehgdb saved!\n",
      "Now preprocessing  ehgdb1\n",
      "signal before trimming/deleting channels: (100000, 16)\n",
      "signal after trimming/deleting channels: (76000, 16)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(normalized_data_path, data) \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mprocess_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# process_one('nifeadb')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mprocess_all\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m trim_data(data, dataset_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal after trimming/deleting channels:\u001b[39m\u001b[38;5;124m'\u001b[39m, data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m z_normalize_signals(data)\n\u001b[1;32m     11\u001b[0m normalized_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(processed_data_path, dataset_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_preprocessed.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(data, bandwidth)\u001b[0m\n\u001b[1;32m     18\u001b[0m     fs \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Apply bandpass filter\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     filtered_signal \u001b[38;5;241m=\u001b[39m \u001b[43mbutter_bandpass_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowcut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbandwidth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighcut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbandwidth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     filtered_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_name\u001b[39m\u001b[38;5;124m'\u001b[39m: entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m: filtered_signal,\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m: entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m     })\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_data\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mbutter_bandpass_filter\u001b[0;34m(data, lowcut, highcut, fs, order)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mApply a Butterworth bandpass filter to the signal.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m b, a \u001b[38;5;241m=\u001b[39m butter(order, highcut, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m, fs\u001b[38;5;241m=\u001b[39mfs)\n\u001b[0;32m----> 6\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfiltfilt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m b, a \u001b[38;5;241m=\u001b[39m butter(order, lowcut, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, fs\u001b[38;5;241m=\u001b[39mfs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtfilt(b, a, y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/signal/_signaltools.py:4540\u001b[0m, in \u001b[0;36mfiltfilt\u001b[0;34m(b, a, x, axis, padtype, padlen, method, irlen)\u001b[0m\n\u001b[1;32m   4537\u001b[0m x0 \u001b[38;5;241m=\u001b[39m axis_slice(ext, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   4539\u001b[0m \u001b[38;5;66;03m# Forward filter.\u001b[39;00m\n\u001b[0;32m-> 4540\u001b[0m (y, zf) \u001b[38;5;241m=\u001b[39m \u001b[43mlfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4542\u001b[0m \u001b[38;5;66;03m# Backward filter.\u001b[39;00m\n\u001b[1;32m   4543\u001b[0m \u001b[38;5;66;03m# Create y0 so zi*y0 broadcasts appropriately.\u001b[39;00m\n\u001b[1;32m   4544\u001b[0m y0 \u001b[38;5;241m=\u001b[39m axis_slice(y, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/signal/_signaltools.py:2177\u001b[0m, in \u001b[0;36mlfilter\u001b[0;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sigtools\u001b[38;5;241m.\u001b[39m_linear_filter(b, a, x, axis)\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sigtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_all(datasets):\n",
    "    for dataset_name in datasets: \n",
    "        print('Now preprocessing ', dataset_name)\n",
    "        data_path = os.path.join(raw_data_path, dataset_name + '_data.npy')\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        print('signal before trimming/deleting channels:', data[0]['signal'].shape)\n",
    "        data = trim_data(data, dataset_name)\n",
    "        print('signal after trimming/deleting channels:', data[0]['signal'].shape)\n",
    "        data = filter_data(data)\n",
    "        data = z_normalize_signals(data)\n",
    "        normalized_data_path = os.path.join(processed_data_path, dataset_name + \"_preprocessed.npy\")\n",
    "        np.save(normalized_data_path, data) \n",
    "        print(dataset_name, 'saved!')\n",
    "\n",
    "def process_one(dataset_name):\n",
    "    print('Now preprocessing ', dataset_name)\n",
    "    data_path = os.path.join(raw_data_path, dataset_name + '_data.npy')\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    print('signal before trimming/deleting channels:', data[0]['signal'].shape)\n",
    "    data = trim_data(data, dataset_name)\n",
    "    print('signal after trimming/deleting channels:', data[0]['signal'].shape)\n",
    "    data = filter_data(data)\n",
    "    data = z_normalize_signals(data)\n",
    "    normalized_data_path = os.path.join(processed_data_path, dataset_name + \"_preprocessed.npy\")\n",
    "    np.save(normalized_data_path, data) \n",
    "    print(dataset_name, 'saved!')\n",
    "\n",
    "process_all(datasets)\n",
    "# process_one('nifeadb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of icehgds\n",
      "Total number of entries before: 126\n",
      "Total number of entries after: 126\n",
      "Max sequence length: 38220\n",
      "Min sequence length: 35040\n",
      "Mean sequence length: 35505.58\n",
      "Max sequence length: 35820\n",
      "Min sequence length: 32640\n",
      "Mean sequence length: 33105.58\n",
      "Statistics of ninfea\n",
      "Total number of entries before: 60\n",
      "Total number of entries after: 60\n",
      "Max sequence length: 245306\n",
      "Min sequence length: 15351\n",
      "Mean sequence length: 62560.15\n",
      "Max sequence length: 204346\n",
      "Min sequence length: 1793\n",
      "Mean sequence length: 37984.15\n",
      "Statistics of nifeadb\n",
      "Total number of entries before: 26\n",
      "Total number of entries after: 26\n",
      "Max sequence length: 961521\n",
      "Min sequence length: 309423\n",
      "Mean sequence length: 600957.08\n",
      "Max sequence length: 901521\n",
      "Min sequence length: 249423\n",
      "Mean sequence length: 490187.85\n"
     ]
    }
   ],
   "source": [
    "print_data = ['icehgds', 'ninfea', 'nifeadb'] # for when running process_all\n",
    "\n",
    "for dataset_name in print_data: \n",
    "    print('Statistics of', dataset_name)\n",
    "    data_path_original = os.path.join(raw_data_path, dataset_name + '_data.npy')\n",
    "    data_original = np.load(data_path_original, allow_pickle=True)\n",
    "    data_path_preprocessed = os.path.join(processed_data_path, dataset_name + \"_preprocessed.npy\")\n",
    "    data_preprocessed = np.load(data_path_preprocessed, allow_pickle=True)\n",
    "\n",
    "    print(f\"Total number of entries before: {len(data_original)}\")\n",
    "    print(f\"Total number of entries after: {len(data_preprocessed)}\")\n",
    "    # Extract sequence lengths\n",
    "    sequence_lengths_original = np.array([entry['signal'].shape[0] for entry in data_original])\n",
    "    sequence_lengths_processed = np.array([entry['signal'].shape[0] for entry in data_preprocessed])\n",
    "\n",
    "    # Compute statistics\n",
    "    print(f\"Max sequence length: {np.max(sequence_lengths_original)}\")\n",
    "    print(f\"Min sequence length: {np.min(sequence_lengths_original)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths_original):.2f}\")\n",
    "    # print(f\"Standard deviation of sequence lengths: {np.std(sequence_lengths):.2f}\")\n",
    "\n",
    "    print(f\"Max sequence length: {np.max(sequence_lengths_processed)}\")\n",
    "    print(f\"Min sequence length: {np.min(sequence_lengths_processed)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths_processed):.2f}\")\n",
    "    # print(f\"Standard deviation of sequence lengths: {np.std(sequence_lengths):.2f}\")\n",
    "    # Check number of channels\n",
    "    # num_channels = set(entry['signal'].shape[1] for entry in data)\n",
    "    # print(f\"Unique number of channels in dataset: {num_channels}\")\n",
    "\n",
    "    # Print a sample metadata entry\n",
    "    # print(\"Sample metadata:\", {k: v for k, v in data[0].items() if k != 'signal'})\n",
    "    # print(data[0]['metadata']['fs'])\n",
    "    # print('Number of channels: ', data[0]['signal'].shape[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
