{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes, trims, and filters NINFEADB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from scipy.signal import butter, filtfilt, sosfiltfilt, decimate\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from config import raw_data_path, univariate_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter datasets you want to preprocess\n",
    "datasets = ['tpehgt', 'tpehgdb', 'ehgdb1', 'ehgdb2', 'icehgds', 'ninfea', 'nifeadb'] # for when running process_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_info(dataset): \n",
    "    data_path = os.path.join(raw_data_path, dataset + '_data.npy')\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "    # Print basic dataset info\n",
    "    print(f\"Total number of entries: {len(data)}\")\n",
    "    print(f\"First entry keys: {list(data[0].keys())}\")  # Check the dictionary structure\n",
    "\n",
    "    # Extract sequence lengths\n",
    "    sequence_lengths = np.array([entry['signal'].shape[0] for entry in data])\n",
    "\n",
    "    # Compute statistics\n",
    "    print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
    "    print(f\"Min sequence length: {np.min(sequence_lengths)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence lengths: {np.std(sequence_lengths):.2f}\")\n",
    "\n",
    "    # Check number of channels\n",
    "    num_channels = set(entry['signal'].shape[1] for entry in data)\n",
    "    print(f\"Unique number of channels in dataset: {num_channels}\")\n",
    "\n",
    "    # Print a sample metadata entry\n",
    "    print(\"Sample metadata:\", {k: v for k, v in data[0].items() if k != 'signal'})\n",
    "    print(data[0]['metadata']['fs'])\n",
    "    print('Number of channels: ', data[0]['signal'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot signals\n",
    "def plot_signals(signal, title='Signals', filename=None):\n",
    "    num_channels = signal.shape[1]\n",
    "    fig, axes = plt.subplots(num_channels, 1, figsize=(12, 2 * num_channels), sharex=True)\n",
    "\n",
    "    for i in range(num_channels):\n",
    "        axes[i].plot(signal[:, i], label=f'Channel {i+1}')\n",
    "        axes[i].legend(loc='upper right')\n",
    "        axes[i].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time Steps\")\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Save the figure\n",
    "    # plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    # plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trim_data(data, dataset_name, sec_to_remove=60):\n",
    "    \"\"\"\n",
    "    Trim the first and last `sec_to_remove` seconds from each sequence in the dataset.\n",
    "    Returns a new dataset with trimmed signals.\n",
    "    \"\"\"\n",
    "    trimmed_data = []\n",
    "    if dataset_name == 'ninfea':\n",
    "        sec_to_remove = 10\n",
    "    \n",
    "    for entry in data:\n",
    "        sampling_frequency = int(entry['metadata']['fs'])\n",
    "        ts_to_remove = sec_to_remove * sampling_frequency\n",
    "        \n",
    "        signal = entry['signal']\n",
    "        trimmed_signal = signal[ts_to_remove:-ts_to_remove] if 2 * ts_to_remove < len(signal) else signal\n",
    "        \n",
    "        # Remove specific channels if dataset is 'ninfea'\n",
    "        if dataset_name == 'ninfea':\n",
    "            channels_to_remove = [27, 28, 29, 30, 32, 33] # Remove channels containing 0.0 values\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "        \n",
    "        # Remove last two channels if dataset is 'tpehgt'\n",
    "        if dataset_name == 'tpehgt':\n",
    "            channels_to_remove = [1, 3, 5, 6, 7] # Remove filtered/TOCO channels (TOCO = channel 6 & 7)\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "\n",
    "        if dataset_name == 'tpehgdb':\n",
    "            channels_to_remove = [1, 2, 3, 5, 6, 7, 9, 10, 11] # Remove filtered channels\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "\n",
    "        if dataset_name == 'icehgds':\n",
    "            channels_to_remove = [1, 3, 5] # Remove filtered channels\n",
    "            trimmed_signal = np.delete(trimmed_signal, channels_to_remove, axis=1)\n",
    "\n",
    "        trimmed_data.append({\n",
    "            'record_name': entry['record_name'],\n",
    "            'signal': trimmed_signal,\n",
    "            'metadata': entry['metadata']\n",
    "        })\n",
    "    \n",
    "    return trimmed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth bandpass filter to the signal.\n",
    "    \"\"\"\n",
    "    b, a = butter(order, highcut, 'high', fs=fs)\n",
    "    y = filtfilt(b, a, data, axis=0)\n",
    "    b, a = butter(order, lowcut, 'low', fs=fs)\n",
    "    return filtfilt(b, a, y, axis=0)\n",
    "\n",
    "def filter_data(data, bandwidth=[0.3, 0.4]):\n",
    "    \"\"\"\n",
    "    Filter each channel of the signal within the specified bandwidth.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        signal = entry['signal']\n",
    "        fs = entry['metadata']['fs']\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        filtered_signal = butter_bandpass_filter(signal, lowcut=bandwidth[0], highcut=bandwidth[1], fs=fs)\n",
    "        \n",
    "        filtered_data.append({\n",
    "            'record_name': entry['record_name'],\n",
    "            'signal': filtered_signal,\n",
    "            'metadata': entry['metadata']\n",
    "        })\n",
    "    \n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def z_normalize_signals(data, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Apply z-normalization to each channel in the multivariate time series dataset.\n",
    "    \"\"\"\n",
    "    normalized_entries = []\n",
    "    \n",
    "    for entry in data:\n",
    "        signal = entry['signal']  # Shape: (sequence_length, num_channels)\n",
    "        \n",
    "        if signal.ndim == 1:\n",
    "            signal = signal[:, np.newaxis]  # Ensure 2D array for consistency\n",
    "        \n",
    "        # Compute mean and std for each channel separately\n",
    "        mu = np.mean(signal, axis=0)\n",
    "        sigma = np.std(signal, axis=0)\n",
    "        \n",
    "        # Normalize each channel\n",
    "        normalized_signal = (signal - mu) / (sigma + epsilon)\n",
    "        \n",
    "        # Store the normalized entry\n",
    "        normalized_entries.append({\n",
    "            'record_name': entry['record_name'],\n",
    "            'signal': normalized_signal,\n",
    "            'metadata': entry['metadata']\n",
    "        })\n",
    "    \n",
    "    return normalized_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_normalize(data, tol=1e-2):\n",
    "    \"\"\"\n",
    "    Check if the normalized dataset has mean ~0 and variance ~1 for each channel.\n",
    "    \"\"\"\n",
    "    all_correct = True\n",
    "    incorrect_entries = []\n",
    "    \n",
    "    for entry in data:\n",
    "        signal = entry['signal']\n",
    "        mean_per_channel = np.mean(signal, axis=0)\n",
    "        std_per_channel = np.std(signal, axis=0)\n",
    "        \n",
    "        incorrect = np.where((np.abs(mean_per_channel) >= tol) | (np.abs(std_per_channel - 1) >= tol))[0]\n",
    "        if len(incorrect) > 0:\n",
    "            all_correct = False\n",
    "            incorrect_entries.append((entry['record_name'], incorrect, mean_per_channel[incorrect], std_per_channel[incorrect]))\n",
    "    \n",
    "    if all_correct:\n",
    "        print(\"Normalization check passed: All channels have mean ≈ 0 and std ≈ 1.\")\n",
    "    else:\n",
    "        print(\"Normalization check failed: Some channels deviate from expected mean and std.\")\n",
    "        for record_name, incorrect, means, stds in incorrect_entries:\n",
    "            print(f\"Record {record_name}: \")\n",
    "            for ch, mean, std in zip(incorrect, means, stds):\n",
    "                print(f\"  Channel {ch}: mean = {mean:.4f}, std = {std:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now preprocessing  tpehgt\n",
      "signal before trimming/deleting channels: (35300, 8)\n",
      "signal after trimming/deleting channels: (32900, 3)\n",
      "tpehgt saved!\n",
      "Now preprocessing  tpehgdb\n",
      "signal before trimming/deleting channels: (35180, 12)\n",
      "signal after trimming/deleting channels: (32780, 3)\n",
      "tpehgdb saved!\n",
      "Now preprocessing  ehgdb1\n",
      "signal before trimming/deleting channels: (100000, 16)\n",
      "signal after trimming/deleting channels: (76000, 16)\n",
      "ehgdb1 saved!\n",
      "Now preprocessing  ehgdb2\n",
      "signal before trimming/deleting channels: (725000, 16)\n",
      "signal after trimming/deleting channels: (701000, 16)\n",
      "ehgdb2 saved!\n",
      "Now preprocessing  icehgds\n",
      "signal before trimming/deleting channels: (35280, 6)\n",
      "signal after trimming/deleting channels: (32880, 3)\n",
      "icehgds saved!\n",
      "Now preprocessing  ninfea\n",
      "signal before trimming/deleting channels: (57490, 34)\n",
      "signal after trimming/deleting channels: (16530, 28)\n",
      "ninfea saved!\n",
      "Now preprocessing  nifeadb\n",
      "signal before trimming/deleting channels: (600052, 6)\n",
      "signal after trimming/deleting channels: (480052, 6)\n",
      "nifeadb saved!\n"
     ]
    }
   ],
   "source": [
    "def process_all(datasets):\n",
    "    for dataset_name in datasets: \n",
    "        print('Now preprocessing ', dataset_name)\n",
    "        data_path = os.path.join(raw_data_path, dataset_name + '_data.npy')\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        print('signal before trimming/deleting channels:', data[0]['signal'].shape)\n",
    "        data = trim_data(data, dataset_name)\n",
    "        print('signal after trimming/deleting channels:', data[0]['signal'].shape)\n",
    "        data = filter_data(data)\n",
    "        data = z_normalize_signals(data)\n",
    "        normalized_data_path = os.path.join(processed_data_path, dataset_name + \"_preprocessed.npy\")\n",
    "        np.save(normalized_data_path, data) \n",
    "        print(dataset_name, 'saved!')\n",
    "\n",
    "def process_one(dataset_name):\n",
    "    print('Now preprocessing ', dataset_name)\n",
    "    data_path = os.path.join(raw_data_path, dataset_name + '_data.npy')\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    print('signal before trimming/deleting channels:', data[0]['signal'].shape)\n",
    "    print('data before trimming: ', data[0])\n",
    "    data = trim_data(data, dataset_name)\n",
    "    print('data after trimming: ', data[0])\n",
    "    print('signal after trimming/deleting channels:', data[0]['signal'].shape)\n",
    "    data = filter_data(data)\n",
    "    data = z_normalize_signals(data)\n",
    "    normalized_data_path = os.path.join(processed_data_path, dataset_name + \"_preprocessed.npy\")\n",
    "    np.save(normalized_data_path, data) \n",
    "    print(dataset_name, 'saved!')\n",
    "\n",
    "process_all(datasets)\n",
    "# process_one('tpehgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of icehgds\n",
      "Total number of entries before: 126\n",
      "Total number of entries after: 126\n",
      "Max sequence length: 38220\n",
      "Min sequence length: 35040\n",
      "Mean sequence length: 35505.58\n",
      "Max sequence length: 35820\n",
      "Min sequence length: 32640\n",
      "Mean sequence length: 33105.58\n",
      "Statistics of ninfea\n",
      "Total number of entries before: 60\n",
      "Total number of entries after: 60\n",
      "Max sequence length: 245306\n",
      "Min sequence length: 15351\n",
      "Mean sequence length: 62560.15\n",
      "Max sequence length: 204346\n",
      "Min sequence length: 1793\n",
      "Mean sequence length: 37984.15\n",
      "Statistics of nifeadb\n",
      "Total number of entries before: 26\n",
      "Total number of entries after: 26\n",
      "Max sequence length: 961521\n",
      "Min sequence length: 309423\n",
      "Mean sequence length: 600957.08\n",
      "Max sequence length: 901521\n",
      "Min sequence length: 249423\n",
      "Mean sequence length: 490187.85\n"
     ]
    }
   ],
   "source": [
    "print_data = ['icehgds', 'ninfea', 'nifeadb'] # for when running process_all\n",
    "\n",
    "for dataset_name in print_data: \n",
    "    print('Statistics of', dataset_name)\n",
    "    data_path_original = os.path.join(raw_data_path, dataset_name + '_data.npy')\n",
    "    data_original = np.load(data_path_original, allow_pickle=True)\n",
    "    data_path_preprocessed = os.path.join(processed_data_path, dataset_name + \"_preprocessed.npy\")\n",
    "    data_preprocessed = np.load(data_path_preprocessed, allow_pickle=True)\n",
    "\n",
    "    print(f\"Total number of entries before: {len(data_original)}\")\n",
    "    print(f\"Total number of entries after: {len(data_preprocessed)}\")\n",
    "    # Extract sequence lengths\n",
    "    sequence_lengths_original = np.array([entry['signal'].shape[0] for entry in data_original])\n",
    "    sequence_lengths_processed = np.array([entry['signal'].shape[0] for entry in data_preprocessed])\n",
    "\n",
    "    # Compute statistics\n",
    "    print(f\"Max sequence length: {np.max(sequence_lengths_original)}\")\n",
    "    print(f\"Min sequence length: {np.min(sequence_lengths_original)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths_original):.2f}\")\n",
    "    # print(f\"Standard deviation of sequence lengths: {np.std(sequence_lengths):.2f}\")\n",
    "\n",
    "    print(f\"Max sequence length: {np.max(sequence_lengths_processed)}\")\n",
    "    print(f\"Min sequence length: {np.min(sequence_lengths_processed)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths_processed):.2f}\")\n",
    "    # print(f\"Standard deviation of sequence lengths: {np.std(sequence_lengths):.2f}\")\n",
    "    # Check number of channels\n",
    "    # num_channels = set(entry['signal'].shape[1] for entry in data)\n",
    "    # print(f\"Unique number of channels in dataset: {num_channels}\")\n",
    "\n",
    "    # Print a sample metadata entry\n",
    "    # print(\"Sample metadata:\", {k: v for k, v in data[0].items() if k != 'signal'})\n",
    "    # print(data[0]['metadata']['fs'])\n",
    "    # print('Number of channels: ', data[0]['signal'].shape[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
